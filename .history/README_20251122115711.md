<div align=center>
<img src="img\logo.png" width="180px">
</div>
<h2 align="center"><a src="paper\medical_embodied_ai.pdf"> Towards Next-Generation Healthcare: A Survey of Medical Embodied AI for Perception, Decision-Making, and Action </a></h2>
<h5 align="center"> If you like our project, please give us a star ‚≠ê on GitHub for the latest update.</h5>

## üè† About

Foundation models have demonstrated impressive performance in enhancing  healthcare efficiency. However, their limited ability to perceive and interact  with the physical world significantly constrains their utility in real-world clinical  workflows. Recently, embodied artificial intelligence (AI) provides a promising  physical-interactive paradigm for intelligent healthcare by integrating percep tion, decision-making, and action within a closed-loop system. Nevertheless, the  exploration of embodied AI for healthcare is still in its infancy. To support these  advances, this review systematically surveys the key components of embodied AI,  focusing on the integration of perception, decision-making, and action. Addition ally, we present a comprehensive overview of representative medical applications,  relevant datasets, major challenges in clinical practice, and further discuss the  key directions for future research in this emerging field. The associated project  can be found at XXXX.

<div align="center">

### [1. Introduction](#1-introduction) | [2. Embodied AI](#2-embodied-ai) |  [3. Embodied AI in Medicine](#3-embodied-ai-in-medicine)

### [4. Datasets and benchmark](#4-datasets-and-benchmark) | [5. Challenges and Outlook](#5-challenges-and-outlook) | [6. Conclusion](#6-conclusion)

</div>

## 1. Introduction

* [2][**Nature Medicine, 2024**] Artificial intelligence in surgery [paper](https://www.nature.com/articles/s41591-024-02970-3)
* [2][**Nature, 2025**] A fully open AI foundation model applied to chest radiography [paper](TBD)
* [3][**Nature Reviews Bioengineering, 2025**] Application of large language models in medicine [paper](TBD)
* [4][**Nature Medicine, 2025**] Toward expert-level medical question answering with large language models [paper](TBD)
* [5][**Nature Medicine, 2025**] A generalist medical language model for disease diagnosis assistance [paper](TBD)
* [6][**IEEE/ASME Transactions on Mechatronics, 2025**] Aligning cyber space with physical world: A comprehensive survey on embodied AI [paper](TBD)
* [7][**ACM Computing Surveys, 2025**] Embodied intelligence: A synergy of morphology, action, perception and learning [paper](TBD)
* [8][**ICML, 2024**] Position: a call for embodied AI [paper](TBD)

## 2. Embodied AI

### 2.1 Embodied Perception

#### 2.1.1 Object Perception

* [1][**IEEE Transactions on Cognitive and Developmental Systems, 2020**] Robot Multimodal Object Perception and Recognition [paper](TBD)
* [2][**IEEE Transactions on Robotics, 2025**] Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference [paper](TBD)
* [3][**NIPS, XXXX**] ImageNet Classification with Deep Convolutional Neural Networks (AlexNet) [paper](TBD)
* [4][**ECCV, 2022**] Open Vocabulary Object Detection with Pseudo Bounding-Box Labels [paper](TBD)
* [5][**CVPR, 2021**] Open-Vocabulary Object Detection Using Captions [paper](TBD)
* [6][**Proceedings of the IEEE, 2002**] Gradient-based learning applied to document recognition [paper](TBD)
* [7][**ICCV, 2021**] Emerging properties in self-supervised vision transformers (DINO) [paper](https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper)
* [8][**arXiv, 2023**] Dinov2: Learning robust visual features without supervision [paper](https://arxiv.org/abs/2304.07193)
* [9][**arXiv, 2025**] DINOv3 [paper](https://arxiv.org/abs/2508.10104)
* [10][**ICCV, 2023**] Segment Anything (SAM) [paper](TBD)
* [11][**arXiv, 2024**] SAM 2: Segment Anything in Images and Videos [paper](TBD)

#### 2.1.2 Scene Perception

* [1][**CVPR, 2025**] Embodied Scene Understanding for Vision Language Models via MetaVQA [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Embodied_Scene_Understanding_for_Vision_Language_Models_via_MetaVQA_CVPR_2025_paper.html)
* [2][**ECCV, 2024**] Embodied Understanding of Driving Scenarios [paper](https://link.springer.com/chapter/10.1007/978-3-031-73033-7_8)
* [3][**ISPRS Journal of Photogrammetry and Remote Sensing, 2024**] Few-shot remote sensing image scene classification [paper](TBD)
* [4][**WACV, 2024**] U3ds3: Unsupervised 3D semantic scene segmentation [paper](https://openaccess.thecvf.com/content/WACV2024/html/Liu_U3DS3_Unsupervised_3D_Semantic_Scene_Segmentation_WACV_2024_paper.html)
* [5][**TPAMI, 2025**] ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments [paper](https://ieeexplore.ieee.org/abstract/document/10495141)
* [6][**ICRA, 2024**] RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation [paper](https://ieeexplore.ieee.org/abstract/document/10610234)
* [7][**arXiv, 2025**] PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era [paper](https://arxiv.org/abs/2509.12989)
* [8][**IROS, 2024**] OmniNxt: A Fully Open-source and Compact Aerial Robot with Omnidirectional Visual Perception [paper](https://ieeexplore.ieee.org/abstract/document/10802134)
* [9][**IROS, 2024**] MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM [paper](https://ieeexplore.ieee.org/abstract/document/10802389)
* [10][**TPAMI, 2022**] Learning View-Based Graph Convolutional Network for Multi-View 3D Shape Analysis [paper](https://ieeexplore.ieee.org/document/9947327)
* [11][**WACV, 2025**] Scene-LLM: Extending Language Model for 3D Visual Reasoning [paper](https://ieeexplore.ieee.org/abstract/document/10943341)
* [12][**ECCV, 2024**] SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding [paper](https://link.springer.com/chapter/10.1007/978-3-031-72673-6_16)

#### 2.1.3 Behavior Perception

* [1][**Expert Systems with Applications, 2024**] Human activity recognition with smartphone-integrated sensors: A survey [paper](https://www.sciencedirect.com/science/article/pii/S0957417424000083)
* [2][**AI Review, 2024**] A survey of video-based human action recognition in team sports [paper](https://link.springer.com/article/10.1007/s10462-024-10934-9)
* [3][**ICRA, 2024**] Anticipate & Act: Integrating LLMs and Classical Planning for Efficient Task Execution [paper](https://ieeexplore.ieee.org/abstract/document/10611164)
* [4][**ICRA, 2025**] CaStL: Constraints as Specifications Through LLM Translation for Long-Horizon Planning [paper](https://ieeexplore.ieee.org/document/11127555)
* [5][**ESWA, 2024**] A new framework for deep learning video-based Human Action Recognition on the edge [paper](https://www.sciencedirect.com/science/article/pii/S0957417423027227)
* [6][**CVPR, 2024**] BlockGCN: Topology Awareness for Skeleton-Based Action Recognition [paper](https://ieeexplore.ieee.org/document/10658569)
* [7][**TIP, 2024**] Learnable Feature Augmentation for Temporal Action Localization [paper](https://ieeexplore.ieee.org/abstract/document/10562229)
* [8][**Pattern Recognition, 2025**] SAM-Net: Semantic-assisted multimodal network for action recognition [paper](https://www.sciencedirect.com/science/article/pii/S0031320325003851)
* [9][**IEEE TIFS, 2025**] Collaboratively Self-Supervised Video Representation Learning for Action Recognition [paper](https://ieeexplore.ieee.org/abstract/document/10847948)

#### 2.1.4 Expression Perception

* [1][**Proceedings of the IEEE, 2023**] Facial Micro-Expressions: An Overview [paper](https://ieeexplore.ieee.org/abstract/document/10144523)
* [2][**IEEE Transactions on Affective Computing, 2023**] Recognition of Human Emotions Using Facial Expressions: A Comprehensive Survey [paper](https://ieeexplore.ieee.org/abstract/document/10041168)
* [3][**IEEE Transactions on Affective Computing, 2020**] Deep facial expression recognition: A survey [paper](https://ieeexplore.ieee.org/abstract/document/9039580)
* [4][**IEEE Transactions on Affective Computing, 2022**] Deep learning for micro-expression recognition: A survey [paper](https://ieeexplore.ieee.org/abstract/document/9915437)
* [5][**TPAMI, 2024**] Prompt tuning of DNNs for speaker-adaptive visual speech recognition [paper](https://ieeexplore.ieee.org/abstract/document/10726873)
* [6][**Pattern Recognition, 2025**] Context transformer with multiscale fusion for robust facial emotion recognition [paper](https://www.sciencedirect.com/science/article/pii/S0031320325003802)
* [7][**IEEE Transactions on Consumer Electronics, 2025**] Cross-Domain Gesture Recognition Using WiFi CSI [paper](https://ieeexplore.ieee.org/abstract/document/10934063)
* [8][**Scientific Data, 2025**] EMG dataset for gesture recognition with arm translation [paper](https://www.nature.com/articles/s41597-024-04296-8)
* [9][**CVPR, 2025**] Uncertain Multimodal Intention and Emotion Understanding in the Wild [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Uncertain_Multimodal_Intention_and_Emotion_Understanding_in_the_Wild_CVPR_2025_paper.html)
* [10][**EMNLP Findings, 2025**] Multimodal Emotion Recognition in Conversations: A Survey [paper](https://arxiv.org/abs/2505.20511)
* [11][**IEEE Transactions on Affective Computing, 2025**] MER-CLIP: AU-Guided Vision-Language Alignment for Micro-Expression Recognition [paper](https://ieeexplore.ieee.org/abstract/document/11060844)

### 2.2 Embodied Decision-Making

#### 2.2.1 Task Planning

* [1][**ACM Computing Surveys, 2023**] Recent Trends in Task and Motion Planning for Robotics: A Survey [paper](https://dl.acm.org/doi/abs/10.1145/3583136)
* [2][**IEEE/ASME Transactions on Mechatronics, 2024**] Optimization-Based Task and Motion Planning: From Classical to Learning [paper](https://ieeexplore.ieee.org/abstract/document/10705419)
* [3][**IROS, 2021**] Learning Symbolic Operators for Task and Motion Planning [paper](https://ieeexplore.ieee.org/abstract/document/9635941)
* [4][**ICRA, 2025**] Delta: Decomposed efficient long-term robot task planning using LLMs [paper](https://ieeexplore.ieee.org/abstract/document/11127838)
* [5][**ICRA, 2025**] Neuro-Symbolic Language Models and Multi-Level Goal Decomposition [paper](https://ieeexplore.ieee.org/abstract/document/11127617)
* [6][**JAIR, 2003**] PDDL2.1: Expressing temporal planning domains [paper](TBD)
* [7][**ICRA, 2025**] Guiding Long-Horizon Task and Motion Planning with VLMs [paper](https://ieeexplore.ieee.org/abstract/document/11128705)

#### 2.2.2 Embodied Navigation

* [1][**Science China Information Sciences, 2025**] Embodied navigation [paper](https://link.springer.com/article/10.1007/s11432-024-4303-8)
* [2][**Information Fusion, 2024**] Embodied navigation with multi-modal information: A survey [paper](https://www.sciencedirect.com/science/article/pii/S1566253524003105)
* [3][**CVPR, 2025**] MNE-SLAM: Multi-Agent Neural SLAM for Mobile Robots [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Deng_MNE-SLAM_Multi-Agent_Neural_SLAM_for_Mobile_Robots_CVPR_2025_paper.html)
* [4][**Information Sciences, 2025**] MAHACO: Hybrid ant colony optimizer for 3D path planning [paper](https://www.sciencedirect.com/science/article/pii/S0020025524016281)
* [5][**IEEE TASE, 2024**] A Survey of Object Goal Navigation [paper](https://ieeexplore.ieee.org/abstract/document/10475904)
* [6][**ICRA, 2024**] Collision Avoidance and Navigation for a Quadrotor Swarm via DRL [paper](https://ieeexplore.ieee.org/abstract/document/10611499)
* [7][**ICRA, 2024**] UIVNAV: Underwater Vision-based Navigation via Imitation Learning [paper](https://ieeexplore.ieee.org/abstract/document/10611203)
* [8][**TPAMI, 2025**] GaussNav: Gaussian Splatting for Visual Navigation [paper](https://ieeexplore.ieee.org/abstract/document/10870413)
* [9][**TPAMI, 2025**] Zero-Shot Vision-Language Navigation in Continuous Environments [paper](https://ieeexplore.ieee.org/abstract/document/11106272)
* [10][**Information Fusion, 2024**] GNN-based multi-agent collaborative navigation [paper](https://www.sciencedirect.com/science/article/pii/S1566253524000289)
* [11][**AAAI, 2025**] NaviFormer: Transformer for Object Navigation [paper](https://ojs.aaai.org/index.php/AAAI/article/view/33612)
* [12][**CVPR, 2025**] Towards Long-Horizon Vision-Language Navigation [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Song_Towards_Long-Horizon_Vision-Language_Navigation_Platform_Benchmark_and_Method_CVPR_2025_paper.html)

#### 2.2.3 Embodied Question Answering (EQA)

* [1][**Information Fusion, 2025**] Embodied Intelligence for 3D Understanding: 3D Scene QA Survey [paper](https://www.sciencedirect.com/science/article/pii/S1566253525006967)
* [2][**CoRL, 2025**] GraphEQA: 3D Semantic Scene Graphs for Real-time EQA [paper](https://openreview.net/forum?id=Yy9EVIajH5)
* [3][**CVPR, 2024**] OpenEQA: Embodied QA in the Era of Foundation Models [paper](https://openaccess.thecvf.com/content/CVPR2024/html/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.html)

### 2.3 Embodied Action

#### 2.3.1 Imitation Learning-Based Action

* [1][**IEEE Transactions on Cybernetics, 2024**] A Survey of Imitation Learning [paper](https://ieeexplore.ieee.org/abstract/document/10602544)
* [2][**Foundations and Trends in Robotics, 2018**] An algorithmic perspective on imitation learning [paper](https://www.nowpublishers.com/article/Details/ROB-053)
* [3][**NeurIPS, 2024**] Is behavior cloning all you need? [paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/da84e39ae51fd26bb5110d9659c06e13-Abstract-Conference.html)
* [4][**IEEE RAL, 2025**] Stable-BC: Controlling Covariate Shift with Stable Behavior Cloning [paper](https://ieeexplore.ieee.org/abstract/document/10829660)
* [5][**AAAI, 2025**] Inverse reinforcement learning by estimating expertise [paper](https://ojs.aaai.org/index.php/AAAI/article/view/33705)
* [6][**ICLR, 2025**] Constraint inference in safety-critical inverse RL [paper](https://openreview.net/forum?id=B2RXwASSpy)
* [7][**IEEE TIE, 2025**] Deep Multimodal Imitation Learning for Robot-Assisted Medical Examination [paper](https://ieeexplore.ieee.org/abstract/document/11122899)
* [8][**ICRA, 2025**] Egomimic: Scaling imitation learning via egocentric video [paper](https://ieeexplore.ieee.org/abstract/document/11127989)

#### 2.3.2 Reinforcement Learning-Based Action

* [1][**AAAI, 2025**] Deep reinforcement learning for robotics: A survey of successes [paper](https://ojs.aaai.org/index.php/AAAI/article/view/35095)
* [2][**IEEE TNNLS, 2022**] Deep reinforcement learning: A survey [paper](https://ieeexplore.ieee.org/abstract/document/9904958)
* [3][**JAIR, 1996**] Reinforcement learning: A survey [paper](https://www.jair.org/index.php/jair/article/view/10166)
* [4][**Machine Learning, 1992**] Q-learning [paper](https://link.springer.com/article/10.1007/bf00992698)
* [5][**arXiv, 2017**] Proximal Policy Optimization (PPO) [paper](https://arxiv.org/abs/1707.06347)
* [6][**ICML, 2016**] Asynchronous methods for deep RL (A3C) [paper](https://proceedings.mlr.press/v48/mniha16.html)
* [7][**arXiv, 2015**] DDPG: Continuous control with deep RL [paper](https://arxiv.org/abs/1509.02971)
* [8][**ICML, 2018**] Soft Actor-Critic (SAC) [paper](https://proceedings.mlr.press/v80/haarnoja18b)
* [9][**AAAI, 2025**] Hierarchical RL: Autonomous option invention [paper](https://ojs.aaai.org/index.php/AAAI/article/view/34163)
* [10][**IEEE TITS, 2025**] Meta-RL for transportation systems [paper](https://ieeexplore.ieee.org/abstract/document/10974402)
* [11][**Nature Machine Intelligence, 2025**] Model-based RL for ultrasound-driven autonomous microrobots [paper](https://www.nature.com/articles/s42256-025-01054-2)

#### 2.3.3 Large Model-Driven Action

* [1][**arXiv, 2023**] GPT-4 Technical Report [paper](https://arxiv.org/abs/2303.08774)
* [2][**arXiv, 2023**] PaLM-E: An Embodied Multimodal Language Model [paper](https://arxiv.org/abs/2303.03378)
* [3][**arXiv, 2022**] Flamingo: a Visual Language Model for Few-Shot Learning [paper](https://arxiv.org/abs/2204.14198)
* [4][**ICML, 2023**] BLIP-2: bootstrapping language-image pre-training [paper](https://dl.acm.org/doi/10.5555/3618408.3619222)
* [5][**arXiv, 2023**] RT-2: Vision-Language-Action Models for Robotic Control [paper](https://arxiv.org/abs/2307.15818)
* [6][**arXiv, 2023**] Code as Policies: Language Model Programs for Embodied Control [paper](https://arxiv.org/abs/2209.07753)
* [7][**arXiv, 2025**] Embodied AI Agents: Modeling the World [paper](https://arxiv.org/abs/2506.22355)

## 3. Embodied AI in Medicine

### 3.1 Medical Embodied Perception

#### 3.1.1 Medical Instrument and Organ Recognition

* [1][**arXiv, 2020**] Deep learning in multi-organ segmentation [paper](https://arxiv.org/abs/2001.10619)
* [2][**Artificial Intelligence Review, 2024**] Surgical instrument recognition and segmentation in robotic-assisted surgeries: a systematic review [paper](https://link.springer.com/article/10.1007/s10462-024-10979-w)
* [3][**arXiv, XXXX**] SwinPA-Net: Transformer-based Multiscale Feature Pyramid Aggregation for Medical Segmentation [paper](TBD)
* [4][**arXiv, XXXX**] ST-MTL: Spatio-Temporal Multitask Learning for instrument tracking [paper](TBD)
* [5][**arXiv, XXXX**] Deep Learning Robotic Tool Detection and Articulation Estimation with Spatio-Temporal Layers [paper](TBD)
* [6][**arXiv, 2025**] SurgVLM: A Large Vision-Language Model for Surgical Intelligence [paper](TBD)

#### 3.1.2 Surgical and Clinical Environment Perception and Modeling

* [1][**IPCAI, 2020**] A Robotic 3D Perception System for Operating Room Environment Awareness [paper](https://arxiv.org/abs/2003.09487)
* [2][**Neurosurgical Focus, 2024**] Virtual operating room: a preliminary study [paper](https://thejns.org/focus/view/journals/neurosurg-focus/56/1/article-pE6.xml)
* [3][**arXiv, XXXX**] NeRF-OR: OR Scene Reconstruction from Sparse-view RGB-D Videos [paper](TBD)
* [4][**arXiv, XXXX**] Deform3DGS: Fast Surgical Scene Reconstruction with Gaussian Splatting [paper](TBD)
* [5][**arXiv, XXXX**] 4D-OR: Semantic Scene Graphs for OR Domain Modeling [paper](TBD)
* [6][**arXiv, XXXX**] LABRAD-OR: Lightweight Memory Scene Graphs for Dynamic ORs [paper](TBD)
* [7][**arXiv, XXXX**] Spatial-ORMLLM: Multimodal LLMs for Spatial Relation Understanding in OR [paper](TBD)

#### 3.1.3 Medical Operation Behavior Detection

* [1][**JBHI, 2023**] Deep learning in surgical workflow analysis: phase and step recognition [paper](https://ieeexplore.ieee.org/abstract/document/10238472)
* [2][**MICCAI, 2019**] 3D CNNs for surgical gesture recognition in video [paper](https://link.springer.com/chapter/10.1007/978-3-030-32254-0_52)
* [3][**IROS, 2022**] Recognition and prediction of surgical gestures using transformer models [paper](https://ieeexplore.ieee.org/abstract/document/9981611)
* [4][**arXiv, XXXX**] Mgrformer: Multimodal Transformer for Surgical Gesture Recognition [paper](TBD)
* [5][**arXiv, XXXX**] Surgical Gesture Recognition in Open Surgery with 3DCNN and SlowFast [paper](TBD)
* [6][**arXiv, XXXX**] TransSG: Spatial-Temporal Transformer for Surgical Gesture Recognition [paper](TBD)
* [7][**arXiv, XXXX**] STANet: Surgical Gesture Recognition via Spatiotemporal Fusion [paper](TBD)
* [8][**arXiv, XXXX**] Audio- and Video-Based Human Activity Recognition Systems in Healthcare [paper](TBD)

#### 3.1.4 Emotional Interaction Understanding

* [1][**IEEE Transactions on Affective Computing, 2020**] Deep facial expression recognition: A survey [paper](https://ieeexplore.ieee.org/abstract/document/9039580)
* [2][**IEEE Transactions on Affective Computing, 2022**] Deep learning for micro-expression recognition: A survey [paper](https://ieeexplore.ieee.org/abstract/document/9915437)
* [3][**Pattern Recognition, 2025**] Multimodal latent emotion recognition from micro-expression and physiological signal [paper](https://www.sciencedirect.com/science/article/pii/S0031320325006235)
* [4][**Information Sciences, 2025**] Towards facial micro-expression detection via multimodal ensemble learning [paper](https://www.sciencedirect.com/science/article/pii/S156625352400513X)
* [5][**arXiv, XXXX**] DEP-former: Multimodal Depression Recognition via Facial and Audio Features [paper](TBD)
* [6][**arXiv, XXXX**] Multimodal Speech Emotion Recognition Using Cross-Attention [paper](TBD)
* [7][**arXiv, XXXX**] Multimodal Fusion of Behavioral and Physiological Signals for Emotion Recognition [paper](TBD)
* [8][**arXiv, XXXX**] Deep Learning-Based Automated Emotion Recognition Using Physiological Signals [paper](TBD)
* [9][**arXiv, XXXX**] MedVLM-R1: Incentivizing medical reasoning capability of VLMs via RL [paper](TBD)
* [10][**arXiv, XXXX**] DialogueLLM: Emotion Knowledge-Tuned LLM for Conversations [paper](TBD)

### 3.2 Medical Embodied Decision-Making

#### 3.2.1 Medical Workflow Modeling and Task Planning

* [1][**EBioMedicine, 2019**] Artificial intelligence to support clinical decision-making processes [paper](https://www.thelancet.com/article/S2352-3964(19)30454-2/fulltext)
* [2][**arXiv, XXXX**] SurgVLM: Systematic Evaluation Benchmark for Surgical Intelligence [paper](TBD)
* [3][**ACM Computing Surveys, 2023**] Recent Trends in Task and Motion Planning for Robotics [paper](https://dl.acm.org/doi/abs/10.1145/3583136)

#### 3.2.2 Medical Navigation Systems

* [1][**IEEE TASE, 2025**] Sim2Real Learning for Autonomous Guidewire Navigation [paper](TBD)
* [2][**IEEE Transactions on Fuzzy Systems, 2025**] Multi-Agent Fuzzy RL with LLM for Endovascular Robotics [paper](TBD)
* [3][**IEEE Transactions on Robotics, 2023**] Autonomous Navigation for Robot-Assisted Intraluminal Procedures: Review [paper](TBD)
* [4][**ICRA, 2025**] VascularPilot3D: Fully Autonomous Navigation for Endovascular Robotics [paper](TBD)
* [5][**ICRA, 2025**] SLAM Assisted 3D Tracking System for Laparoscopic Surgery [paper](TBD)
* [6][**CISS, 2025**] Informative Path Planning for Nano-Surgical Robot Drug Delivery [paper](TBD)

#### 3.2.3 Clinical Question-answering and Decision Support

* [1][**CVPR, 2024**] OpenEQA: Embodied Question Answering in the Era of Foundation Models [paper](https://openaccess.thecvf.com/content/CVPR2024/html/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.html)
* [2][**CoRL, 2025**] GraphEQA: 3D Semantic Scene Graphs for Real-time EQA [paper](https://openreview.net/forum?id=Yy9EVIajH5)
* [3][**Nature Medicine, 2025**] Generalist medical language model for diagnosis assistance [paper](TBD)
* [4][**Nature, 2025**] Fully open AI foundation model applied to chest radiography [paper](TBD)
* [5][**Nature Reviews Bioengineering, 2025**] Application of LLMs in medicine [paper](TBD)
* [6][**Nature Medicine, 2025**] Expert-level medical QA with LLMs [paper](TBD)

### 3.3 Medical Embodied Action

#### 3.3.1 Medical Imitation-based Action

* [1][**IEEE TIE, 2025**] Deep Multimodal Imitation Learning for Robot-Assisted Medical Examination [paper](https://ieeexplore.ieee.org/abstract/document/11122899)
* [2][**AIM, 2025**] Demonstration-based Therapist Skill Transfer for Upper-Limb Exoskeleton [paper](TBD)
* [3][**ICRA, 2025**] Egomimic: Scaling imitation learning via egocentric video [paper](https://ieeexplore.ieee.org/abstract/document/11127989)
* [4][**NeurIPS, 2024**] Behavior cloning horizon in imitation learning [paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/da84e39ae51fd26bb5110d9659c06e13-Abstract-Conference.html)

#### 3.3.2 Medical Reinforcement-based Action

* [1][**Nature Machine Intelligence, 2025**] Model-based RL for ultrasound-driven autonomous microrobots [paper](https://www.nature.com/articles/s42256-025-01054-2)
* [2][**Frontiers in Bioengineering and Biotechnology, 2025**] AI-driven hybrid rehabilitation for stroke recovery [paper](TBD)
* [3][**IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2025**] RL Methods for Assistive and Rehabilitation Robotics: A Survey [paper](TBD)
* [4][**Nature, 2024**] Experiment-free exoskeleton assistance via learning in simulation [paper](TBD)

#### 3.3.3 Medical Large Model-Driven Action

* [1][**arXiv, 2025**] MedVLM-R1: Incentivizing medical reasoning capability of VLMs via RL [paper](TBD)
* [2][**arXiv, 2025**] SurgVLM: A Large Vision-Language Model for Surgical Intelligence [paper](TBD)
* [3][**Nature Medicine, 2025**] Generalist medical language model for diagnosis assistance [paper](TBD)

### 3.4 Integrated Application Scenarios in Healthcare

#### 3.4.1 Surgical Robot

* [1][**Science Robotics, 2025**] Surgical embodied intelligence for task autonomy in laparoscopy [paper](TBD)
* [2][**Proceedings of the IEEE, 2022**] Concepts and Trends in Autonomy for Robot-Assisted Surgery [paper](TBD)
* [3][**Nature Reviews Bioengineering, 2025**] Robotic surgery [paper](TBD)
* [4][**IEEE Transactions on Robotics, 2024**] Miniaturizing Vision-Based Tactile Sensors with Fiber Optic Bundles [paper](TBD)

#### 3.4.2 Intelligent Caregiving and Companion Robot

* [1][**IEEE Transactions on Affective Computing, 2025**] Affective Embodied Agent for Patient Assistance in Virtual Rehabilitation [paper](TBD)
* [2][**i-CREATe, 2024**] Virtual Co-Embodiment Rehabilitation integrating Action Observation Therapy [paper](TBD)
* [3][**JMIR, 2024**] Embodied Conversational Agents for Chronic Diseases: Scoping Review [paper](TBD)
* [4][**APMS, 2020**] Autonomous Mobile Robots in Hospital Logistics [paper](TBD)
* [5][**Autonomous Robots, 2024**] Comprehensive classification of risks for hospital robotic assistance [paper](TBD)
* [6][**ICSR, 2025**] Interaction Matters for Hand Disinfection Using Robots at Hospitals [paper](TBD)
* [7][**IC-BIS, 2024**] Multirobot collaboration for medical consumables logistics in OR [paper](TBD)

#### 3.4.3 Immersive Medical Education Platform

* [1][**TBD, XXXX**] Placeholder for immersive medical education platform references [paper](TBD)

#### 3.4.4 Telecollaborative Diagnostic and Treatment System

* [1][**TBD, XXXX**] Placeholder for telecollaborative diagnostic and treatment system references [paper](TBD)

## 4. Datasets and benchmark

### 4.1 Perception Datasets

#### 4.1.1 Organ‚ÄìInstrument Recognition Datasets

* [1][**TBD, XXXX**] Placeholder for organ‚Äìinstrument recognition datasets [paper](TBD)

#### 4.1.2 Medical Scene Modeling Datasets

* [1][**TBD, XXXX**] Placeholder for medical scene modeling datasets [paper](TBD)

#### 4.1.3 Clinical Action and Pose Estimation Datasets

* [1][**Scientific Data, 2025**] EMG dataset for gesture recognition with arm translation [paper](https://www.nature.com/articles/s41597-024-04296-8)

#### 4.1.4 Multimodal Affective Perception Datasets

* [1][**TBD, XXXX**] Placeholder for multimodal affective perception datasets [paper](TBD)

### 4.2 Decision-Making Datasets

* [1][**TBD, XXXX**] Placeholder for decision-making datasets [paper](TBD)

#### 4.2.1 Surgical Workflow Annotation Datasets

#### 4.2.2 Medical Navigation Datasets

#### 4.2.3 Medical Question Answering Datasets

### 4.3 Action Datasets

* [1][**TBD, XXXX**] Placeholder for action datasets [paper](TBD)

### 4.4 Simulation Platforms and Synthetic Datasets

#### 4.4.1 Surgical Simulation Platforms

* [1][**TBD, XXXX**] Placeholder for surgical simulation platforms [paper](TBD)

#### 4.4.2 Synthetic Datasets

* [1][**TBD, XXXX**] Placeholder for synthetic datasets [paper](TBD)

## 5. Challenges and Outlook

### 5.1 Insufficient Training Data and Perception Discrepancy

* [1][**TBD, XXXX**] Placeholder reference related to data insufficiency [paper](TBD)

### 5.2 Semantic Ambiguity and Multimodal Knowledge Fusion  Difficulties

* [1][**TBD, XXXX**] Placeholder reference related to semantic ambiguity [paper](TBD)

### 5.3 Medical Reasoning Complexity and Uncertainty Modeling

* [1][**arXiv, 2025**] Towards Robust and Secure Embodied AI: Vulnerabilities and Attacks [paper](TBD)

### 5.4 Lack of Mechanisms for Decision Pathway Generation and  Validation

* [1][**TBD, XXXX**] Placeholder reference related to decision pathways [paper](TBD)

### 5.5 Error Sensitivity in High-Precision Action Control

* [1][**TBD, XXXX**] Placeholder reference related to precision action control [paper](TBD)

### 5.6 Lack of General-Purpose Medical Simulation Platforms

* [1][**TBD, XXXX**] Placeholder reference related to medical simulation platforms [paper](TBD)

## 6. Conclusion

