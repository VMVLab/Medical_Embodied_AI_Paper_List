<div align=center>
<img src="img\logo.png" width="180px">
</div>
<h2 align="center"><a src="paper\medical_embodied_ai.pdf"> Towards Next-Generation Healthcare: A Survey of Medical Embodied AI for Perception, Decision-Making, and Action </a></h2>
<h5 align="center"> If you like our project, please give us a star ‚≠ê on GitHub for the latest update.</h5>

## üè† About

Foundation models have demonstrated impressive performance in enhancing  healthcare efficiency. However, their limited ability to perceive and interact  with the physical world significantly constrains their utility in real-world clinical  workflows. Recently, embodied artificial intelligence (AI) provides a promising  physical-interactive paradigm for intelligent healthcare by integrating percep tion, decision-making, and action within a closed-loop system. Nevertheless, the  exploration of embodied AI for healthcare is still in its infancy. To support these  advances, this review systematically surveys the key components of embodied AI,  focusing on the integration of perception, decision-making, and action. Addition ally, we present a comprehensive overview of representative medical applications,  relevant datasets, major challenges in clinical practice, and further discuss the  key directions for future research in this emerging field. The associated project  can be found at XXXX.

<div align="center">

### [1. Introduction](#1-introduction) | [2. Embodied AI](#2-embodied-ai) |  [3. Embodied AI in Medicine](#3-embodied-ai-in-medicine)

### [4. Datasets and benchmark](#4-datasets-and-benchmark) | [5. Challenges and Outlook](#5-challenges-and-outlook) | [6. Conclusion](#6-conclusion)

</div>

## 1. Introduction

* ‚Äã**[1][**Nature Medicine, 2024**] Artificial intelligence in surgery** [paper](https://www.nature.com/articles/s41591-024-02970-3)
* ‚Äã[2][**Nature Medicine, 2023**] A deep learning algorithm to classify skin lesions from mpox virus infection [paper](XXX)
* ‚Äã[3][**Nature Methods, 2021**] nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation [paper](XXX)
* ‚Äã[4][**Nature, 2025**] A fully open AI foundation model applied to chest radiography [paper](XXX)
* ‚Äã[5][**Nature Reviews Bioengineering, 2025**] Application of large language models in medicine [paper](XXX)
* ‚Äã[6][**Nature Medicine, 2025**] Toward expert-level medical question answering with large language models [paper](XXX)
* ‚Äã[7][**Nature Medicine, 2025**] A generalist medical language model for disease diagnosis assistance [paper](XXX)
* ‚Äã[8][**IEEE/ASME Transactions on Mechatronics, 2025**] Aligning cyber space with physical world: A comprehensive survey on embodied AI [paper](XXX)
* ‚Äã[9][**Nature Communications, 2025**] AI-embodied multi-modal flexible electronic robots with programmable sensing, actuating and self-learning [paper](XXX)
* ‚Äã[10][**Science Robotics, 2025**] Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery [paper](XXX)
* ‚Äã[11][**Proceedings of the IEEE, 2022**] Concepts and trends in autonomy for robot-assisted surgery [paper](XXX)
* ‚Äã[12][**IEEE Transactions on Automation Science and Engineering, 2025**] Sim2real learning with domain randomization for autonomous guidewire navigation in robotic-assisted endovascular procedures [paper](XXX)
* ‚Äã[13][**IEEE Transactions on Fuzzy Systems, 2025**] Multi-agent fuzzy reinforcement learning with LLM for cooperative navigation of endovascular robotics [paper](XXX)
* ‚Äã[14][**IEEE Transactions on Robotics, 2023**] Autonomous navigation for robot-assisted intraluminal and endovascular procedures: A systematic review [paper](XXX)
* ‚Äã[15][**ICRA, 2025**] VascularPilot3D: Toward a 3D fully autonomous navigation for endovascular robotics [paper](XXX)
* ‚Äã[16][**ICRA, 2025**] SLAM assisted 3D tracking system for laparoscopic surgery [paper](XXX)
* ‚Äã[17][**CISS, 2025**] Informative path planning for nano-surgical robot adaptive drug delivery [paper](XXX)
* ‚Äã[18][**IEEE Transactions on Affective Computing, 2025**] Affective embodied agent for patient assistance in virtual rehabilitation [paper](XXX)
* ‚Äã[19][**i-CREATe, 2024**] Virtual co-embodiment rehabilitation: An innovative method integrating virtual co-embodiment and action observation therapy in virtual reality rehabilitation [paper](XXX)
* ‚Äã[20][**Journal of Medical Internet Research, 2024**] Embodied conversational agents for chronic diseases: scoping review [paper](XXX)
* ‚Äã[21][**Journal of British Surgery, 2015**] Robotic surgery [paper](XXX)
* ‚Äã[22][**IEEE Transactions on Robotics, 2024**] Using fiber optic bundles to miniaturize vision-based tactile sensors [paper](XXX)
* ‚Äã[23][**Nature, 2024**] Experiment-free exoskeleton assistance via learning in simulation [paper](XXX)
* ‚Äã[24][**IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2025**] Reinforcement learning methods for assistive and rehabilitation robotic systems: A survey [paper](XXX)
* ‚Äã**[25][**AIM, 2025**] Based therapist skill transfer learning framework for upper-limb rehabilitation exoskeleton** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[26][**Frontiers in Bioengineering and Biotechnology, 2025**] AI-driven hybrid rehabilitation: synergizing robotics and electrical stimulation for upper-limb recovery after stroke** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[27][**APMS, 2020**] Autonomous mobile robots in hospital logistics** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[28][**Autonomous Robots, 2024**] Boosting the hospital by integrating mobile robotic assistance systems: a comprehensive classification of the risks to be addressed** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[29][**IC-BIS, 2024**] Research on multirobot collaboration platform for logistic distribution of medical consumables in the operating room** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[30][**ICSR, 2024**] Interaction matters when it comes to hand disinfection using robots at hospitals** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[31][**arXiv, 2025**] Large model empowered embodied AI: A survey on decision-making and embodied learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[32][**arXiv, 2025**] Towards robust and secure embodied AI: A survey on vulnerabilities and attacks** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[33][**arXiv, 2025**] UAVs meet agentic AI: A multidomain survey of autonomous aerial intelligence and agentic UAVs** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[34][**arXiv, 2025**] Embodied AI: From LLMs to world models** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[35][**DTPI, 2024**] Embodied intelligent driving: Key technologies and applications** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[36][**arXiv, 2025**] Multi-agent autonomous driving systems with large language models: A survey of recent advances** [paper](https://www.google.com/url?sa=E&q=placeholder)
* ‚Äã[37][**Information Fusion, 2025**] From screens to scenes: A survey of embodied AI in healthcare [paper](XXX)

## 2. Embodied AI

* ‚Äã[38][**Mind, 1950**] Computing machinery and intelligence [paper](XXX)
* ‚Äã[39][**arXiv, 2025**] Neural brain: A neuroscience-inspired framework for embodied agents [paper](XXX)
* ‚Äã[40][**ACM Computing Surveys, 2025**] Embodied intelligence: A synergy of morphology, action, perception and learning [paper](XXX)
* ‚Äã[41][**ICML, 2024**] Position: a call for embodied AI [paper](XXX)
* ‚Äã[42][**arXiv, 2025**] Embodiedreamer: Advancing real2sim2real transfer for policy training via embodied world modeling [paper](XXX)
* ‚Äã[43][**arXiv, 2024**] Dexsim2real^2: Building explicit world model for precise articulated object dexterous manipulation [paper](XXX)
* ‚Äã[44][**arXiv, 2025**] Bridging the sim2real gap: Vision encoder pre-training for visuomotor policy transfer [paper](XXX)
* ‚Äã[45][**ICML, 2024**] Dexscale: Automating data scaling for sim2real generalizable robot control [paper](XXX)

### 2.1 Embodied Perception

* ‚Äã[90][**CVPR, 2024**] Evidential active recognition: Intelligent and prudent open-world embodied perception [paper](XXX)
* ‚Äã[91][**CVPR, 2024**] Embodiedscan: A holistic multi-modal 3d perception suite towards embodied AI [paper](XXX)
* ‚Äã[92][**Information Fusion, 2024**] Advancements in perception system with multi-sensor fusion for embodied agents [paper](XXX)
* ‚Äã[93][**Information Fusion, 2025**] Tactile data generation and applications based on visuo-tactile sensors: A review [paper](XXX)
  
  ‚Äã

#### 2.1.1 Object Perception

* ‚Äã[94][**IEEE Transactions on Cognitive and Developmental Systems, 2020**] Robot multimodal object perception and recognition: Synthetic maturation of sensorimotor learning in embodied systems [paper](XXX)
* ‚Äã[95][**IEEE Transactions on Robotics, 2025**] Predictive visuo-tactile interactive perception framework for object properties inference [paper](XXX)
* ‚Äã[96][**Proceedings of the IEEE, 2002**] Gradient-based learning applied to document recognition [paper](XXX)
* ‚Äã[97][**NeurIPS, 2017**] Attention is all you need [paper](XXX)
* ‚Äã[98][**ICCV, 2023**] Segment anything [paper](XXX)
* ‚Äã[99][**arXiv, 2024**] Sam 2: Segment anything in images and videos [paper](XXX)
  
  ‚Äã
* ‚Äã**[100][**ICCV, 2021**] Emerging properties in self-supervised vision transformers** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[101][**arXiv, 2023**] Dinov2: Learning robust visual features without supervision** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[102][**arXiv, 2025**] Dinov3** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[46][**NeurIPS, 2012**] Imagenet classification with deep convolutional neural networks** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[47][**arXiv, 2014**] Very deep convolutional networks for large-scale image recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[48][**CVPR, 2016**] Deep residual learning for image recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[49][**NeurIPS, 2015**] Faster R-CNN: Towards real-time object detection with region proposal networks** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[50][**CVPR, 2016**] You only look once: Unified, real-time object detection** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[51][**MICCAI, 2015**] U-net: Convolutional networks for biomedical image segmentation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã[52][**Seminal Graphics Papers, 2023**] SMPL: A skinned multi-person linear model [paper](XXX)
* ‚Äã[53][**ACCV, 2014**] 3d human pose estimation from monocular images with deep convolutional neural network [paper](XXX)
* ‚Äã[54][**CVPR, 2021**] Open-vocabulary object detection using captions [paper](XXX)
* ‚Äã[55][**ECCV, 2022**] Open vocabulary object detection with pseudo bounding-box labels [paper](XXX)
  
  ‚Äã

#### 2.1.2 Scene Perception

* ‚Äã[103][**CVPR, 2025**] Embodied scene understanding for vision language models via metavqa [paper](XXX)
* ‚Äã[104][**ECCV, 2024**] Embodied understanding of driving scenarios [paper](XXX)
* ‚Äã[105][**WACV, 2025**] Scene-llm: Extending language model for 3d visual reasoning [paper](XXX)
* ‚Äã[106][**ECCV, 2024**] Sceneverse: Scaling 3d vision-language learning for grounded scene understanding [paper](XXX)
* ‚Äã[107][**IROS, 2024**] Mm3dgs slam: Multi-modal 3d gaussian splatting for slam using vision, depth, and inertial measurements [paper](XXX)
* ‚Äã[108][**IEEE TPAMI, 2022**] Learning view-based graph convolutional network for multi-view 3d shape analysis [paper](XXX)
* ‚Äã[56][**ISPRS Journal of Photogrammetry and Remote Sensing, 2024**] Few-shot remote sensing image scene classification: Recent advances, new baselines, and future trends [paper](XXX)
* ‚Äã[57][**WACV, 2024**] U3ds3: Unsupervised 3d semantic scene segmentation [paper](XXX)
* ‚Äã[58][**IEEE TPAMI, 2024**] Etpnav: Evolving topological planning for vision-language navigation in continuous environments [paper](XXX)
* ‚Äã[59][**ICRA, 2024**] Robohop: Segment-based topological map representation for open-world visual navigation [paper](XXX)
* ‚Äã[60][**arXiv, 2025**] Panorama: The rise of omnidirectional vision in the embodied AI era [paper](XXX)
* ‚Äã[61][**IROS, 2024**] Omninxt: A fully open-source and compact aerial robot with omnidirectional visual perception [paper](XXX)
  
  ‚Äã

#### 2.1.3 Behavior Perception

* ‚Äã[109][**Expert Systems with Applications, 2024**] Human activity recognition with smartphone-integrated sensors: A survey [paper](XXX)
* ‚Äã[110][**Artificial Intelligence Review, 2024**] A survey of video-based human action recognition in team sports [paper](XXX)
* ‚Äã[111][**Expert Systems with Applications, 2024**] A new framework for deep learning video based human action recognition on the edge [paper](XXX)
* ‚Äã[112][**CVPR, 2024**] Blockgcn: Redefine topology awareness for skeleton-based action recognition [paper](XXX)
* ‚Äã[113][**IEEE Transactions on Image Processing, 2024**] Learnable feature augmentation framework for temporal action localization [paper](XXX)
* ‚Äã[114][**Pattern Recognition, 2025**] Sam-net: Semantic-assisted multimodal network for action recognition in rgb-d videos [paper](XXX)
* ‚Äã[115][**IEEE Transactions on Information Forensics and Security, 2025**] Collaboratively self-supervised video representation learning for action recognition [paper](XXX)
* ‚Äã[62][**ICRA, 2024**] Anticipate & act: Integrating LLMs and classical planning for efficient task execution in household environments [paper](XXX)
* ‚Äã[63][**ICRA, 2025**] Castl: Constraints as specifications through LLM translation for long-horizon task and motion planning [paper](XXX)
  
  ‚Äã

#### 2.1.4 Expression Perception

* ‚Äã**[116][**Proceedings of the IEEE, 2023**] Facial micro-expressions: An overview** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[117][**IEEE Transactions on Instrumentation and Measurement, 2023**] Understanding deep learning techniques for recognition of human emotions using facial expressions: A comprehensive survey** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[118][**arXiv, 2025**] Multimodal emotion recognition in conversations: A survey of methods, trends, challenges and prospects** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[119][**IEEE Transactions on Affective Computing, 2025**] Mer-clip: Au-guided vision-language alignment for micro-expression recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[64][**IEEE TPAMI, 2024**] Prompt tuning of deep neural networks for speaker-adaptive visual speech recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[65][**Pattern Recognition, 2025**] Context transformer with multiscale fusion for robust facial emotion recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[66][**IEEE Transactions on Consumer Electronics, 2025**] Meta-transfer learning based cross-domain gesture recognition using wifi channel state information** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[67][**Scientific Data, 2025**] EMG dataset for gesture recognition with arm translation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[68][**CVPR, 2025**] Uncertain multimodal intention and emotion understanding in the wild** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

### 2.2 Embodied Decision-Making

* ‚Äã**[120][**arXiv, 2025**] A comprehensive survey on multi-agent cooperative decision-making: Scenarios, approaches, challenges and perspectives** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 2.2.1 Task Planning

* ‚Äã**[121][**ACM Computing Surveys, 2023**] Recent trends in task and motion planning for robotics: A survey** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[122][**IEEE/ASME Transactions on Mechatronics, 2024**] A survey of optimization-based task and motion planning: From classical to learning approaches** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã[123][**ICRA, 2025**] Guiding long-horizon task and motion planning with vision language models [paper](XXX)
* ‚Äã[69][**IROS, 2021**] Learning symbolic operators for task and motion planning [paper](XXX)
* ‚Äã[70][**Journal of Artificial Intelligence Research, 2003**] PDDL2.1: An extension to PDDL for expressing temporal planning domains [paper](XXX)
* ‚Äã[71][**ICRA, 2025**] Delta: Decomposed efficient long-term robot task planning using large language models [paper](XXX)
* ‚Äã[72][**ICRA, 2025**] Fast and accurate task planning using neuro-symbolic language models and multi-level goal decomposition [paper](XXX)
  
  ‚Äã

#### 2.2.2 Embodied Navigation

* ‚Äã[124][**Science China Information Sciences, 2025**] Embodied navigation [paper](XXX)
* ‚Äã[125][**Information Fusion, 2024**] Embodied navigation with multi-modal information: A survey from tasks to methodology [paper](XXX)
* ‚Äã[126][**CVPR, 2025**] Mne-slam: Multi-agent neural slam for mobile robots [paper](XXX)
* ‚Äã[127][**Information Sciences, 2025**] Mahaco: Multi-algorithm hybrid ant colony optimizer for 3d path planning of a group of uavs [paper](XXX)
* ‚Äã[128][**IEEE Transactions on Automation Science and Engineering, 2024**] A survey of object goal navigation [paper](XXX)
* ‚Äã[129][**ICRA, 2024**] Collision avoidance and navigation for a quadrotor swarm using end-to-end deep reinforcement learning [paper](XXX)
* ‚Äã[130][**ICRA, 2024**] Uivnav: Underwater information-driven vision-based navigation via imitation learning [paper](XXX)
* ‚Äã[131][**Information Fusion, 2024**] Macns: A generic graph neural network integrated deep reinforcement learning based multi-agent collaborative navigation system for dynamic trajectory planning [paper](XXX)
* ‚Äã[132][**AAAI, 2025**] Naviformer: A spatio-temporal context-aware transformer for object navigation [paper](XXX)
* ‚Äã[133][**CVPR, 2025**] Towards long-horizon vision-language navigation: Platform, benchmark and method [paper](XXX)
* ‚Äã[73][**IEEE TPAMI, 2025**] Gaussnav: Gaussian splatting for visual navigation [paper](XXX)
* ‚Äã[74][**IEEE TPAMI, 2025**] Constraint-aware zero-shot vision-language navigation in continuous environments [paper](XXX)
  
  ‚Äã

#### 2.2.3 Embodied Question Answering (EQA)

* ‚Äã[134][**arXiv, 2025**] Embodied intelligence for 3d understanding: A survey on 3d scene question answering [paper](XXX)
* ‚Äã[75][**arXiv, 2024**] GraphEQA: Using 3d semantic scene graphs for real-time embodied question answering [paper](XXX)
* ‚Äã[76][**CVPR, 2024**] OpenEQA: Embodied question answering in the era of foundation models [paper](XXX)
  
  ‚Äã

### 2.3 Embodied Action

#### 2.3.1 Imitation Learning-Based Action

* ‚Äã[135][**IEEE Transactions on Cybernetics, 2024**] A survey of imitation learning: Algorithms, recent developments, and challenges [paper](XXX)
* ‚Äã[136][**Foundations and Trends in Robotics, 2018**] An algorithmic perspective on imitation learning [paper](XXX)
* ‚Äã[137][**IEEE Transactions on Industrial Electronics, 2025**] Deep multimodal imitation learning-based framework for robot-assisted medical examination [paper](XXX)
* ‚Äã[138][**ICRA, 2025**] Egomimic: Scaling imitation learning via egocentric video [paper](XXX)
* ‚Äã[77][**NeurIPS, 2024**] Is behavior cloning all you need? understanding horizon in imitation learning [paper](XXX)
* ‚Äã[78][**IEEE Robotics and Automation Letters, 2025**] Stable-bc: Controlling covariate shift with stable behavior cloning [paper](XXX)
* ‚Äã[79][**AAAI, 2025**] Inverse reinforcement learning by estimating expertise of demonstrators [paper](XXX)
* ‚Äã[80][**ICLR, 2025**] Understanding constraint inference in safety-critical inverse reinforcement learning [paper](XXX)
  
  ‚Äã

#### 2.3.2 Reinforcement Learning-Based Action

* ‚Äã[139][**AAAI, 2025**] Deep reinforcement learning for robotics: A survey of real-world successes [paper](XXX)
* ‚Äã[140][**IEEE Transactions on Neural Networks and Learning Systems, 2022**] Deep reinforcement learning: A survey [paper](XXX)
* ‚Äã[141][**Journal of Artificial Intelligence Research, 1996**] Reinforcement learning: A survey [paper](XXX)
* ‚Äã[142][**AAAI, 2025**] Autonomous option invention for continual hierarchical reinforcement learning and planning [paper](XXX)
* ‚Äã[143][**IEEE Transactions on Intelligent Transportation Systems, 2025**] Toward adaptive and coordinated transportation systems: A multi-personality multi-agent meta-reinforcement learning framework [paper](XXX)
* ‚Äã[144][**Nature Machine Intelligence, 2025**] Model-based reinforcement learning for ultrasound-driven autonomous microrobots [paper](XXX)
* ‚Äã[81][**Artificial Intelligence for Engineers, 2025**] Value-based reinforcement learning [paper](XXX)
* ‚Äã[82][**Machine Learning, 1992**] Q-learning [paper](XXX)
* ‚Äã[83][**arXiv, 2017**] Proximal policy optimization algorithms [paper](XXX)
* ‚Äã[84][**ICML, 2016**] Asynchronous methods for deep reinforcement learning [paper](XXX)
* ‚Äã[85][**arXiv, 2015**] Continuous control with deep reinforcement learning [paper](XXX)
* ‚Äã[86][**ICML, 2018**] Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor [paper](XXX)
  
  ‚Äã

#### 2.3.3 Large Model-Driven Action

* ‚Äã[87][**arXiv, 2023**] GPT-4 technical report [paper](XXX)
* ‚Äã[88][**NeurIPS, 2022**] Flamingo: a visual language model for few-shot learning [paper](XXX)
* ‚Äã[89][**CoRL, 2023**] Rt-2: Vision-language-action models transfer web knowledge to robotic control [paper](XXX)
* ‚Äã[145][**arXiv, 2023**] Palm-e: An embodied multimodal language model [paper](XXX)
* ‚Äã[146][**ICML, 2023**] Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models [paper](XXX)
* ‚Äã[147][**arXiv, 2022**] Code as policies: Language model programs for embodied control [paper](XXX)
* ‚Äã[148][**arXiv, 2025**] Embodied AI agents: Modeling the world [paper](XXX)
  
  ‚Äã

## 3. Embodied AI in Medicine

### 3.1 Medical Embodied Perception

#### 3.1.1 Medical Instrument and Organ Recognition

* ‚Äã**[149][**arXiv, 2020**] Deep learning in multi-organ segmentation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[150][**Artificial Intelligence Review, 2024**] Deep learning for surgical instrument recognition and segmentation in robotic-assisted surgeries: a systematic review** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[151][**IEEE Transactions on Neural Networks and Learning Systems, 2022**] SwinPA-Net: Swin transformer-based multiscale feature pyramid aggregation network for medical image segmentation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[152][**Medical Image Analysis, 2021**] ST-MTL: Spatio-temporal multitask learning model to predict scanpath while tracking instruments in robotic surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[153][**IEEE Robotics and Automation Letters, 2019**] Deep learning based robotic tool detection and articulation estimation with spatio-temporal layers** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[154][**arXiv, 2025**] SurgVLM: A large vision-language model and systematic evaluation benchmark for surgical intelligence** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.1.2 Surgical and Clinical Environment Perception and Modeling

* ‚Äã**[155][**arXiv, 2020**] A robotic 3D perception system for operating room environment awareness** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[156][**Neurosurgical Focus, 2024**] Creation of a microsurgical neuroanatomy laboratory and virtual operating room: a preliminary study** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[157][**IJCARS, 2025**] NeRF-OR: neural radiance fields for operating room scene reconstruction from sparse-view RGB-D videos** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[158][**MICCAI, 2024**] Deform3DGS: Flexible deformation for fast surgical scene reconstruction with gaussian splatting** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[159][**MICCAI, 2022**] 4D-OR: Semantic scene graphs for or domain modeling** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[160][**MICCAI, 2023**] Labrad-OR: Lightweight memory scene graphs for accurate bimodal reasoning in dynamic operating rooms** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[161][**arXiv, 2025**] Spatial-ORMLLM: Improve spatial relation understanding in the operating room with multimodal large language model** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.1.3 Medical Operation Behavior Detection

* ‚Äã**[162][**IEEE JBHI, 2023**] Deep learning in surgical workflow analysis: a review of phase and step recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[163][**IEEE FG, 2024**] MGRFormer: A multimodal transformer approach for surgical gesture recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[164][**ITNEC, 2024**] Surgical gesture recognition in open surgery based on 3DCNN and SlowFast** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[165][**CGI, 2024**] TransSG: A spatial-temporal transformer for surgical gesture recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[166][**Annals of the New York Academy of Sciences, 2025**] STANet: A surgical gesture recognition method based on spatiotemporal fusion** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[167][**IEEE Access, 2024**] Audio-and video-based human activity recognition systems in healthcare** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[168][**IEEE Transactions on Medical Imaging, 2022**] Gesture recognition in robotic surgery with multimodal attention** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.1.4 Emotional Interaction Understanding

* ‚Äã**[169][**IEEE Transactions on Affective Computing, 2020**] Deep facial expression recognition: A survey** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[170][**IEEE Transactions on Affective Computing, 2022**] Deep learning for micro-expression recognition: A survey** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[171][**Pattern Recognition, 2025**] Multimodal latent emotion recognition from micro-expression and physiological signal** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[172][**Information Fusion, 2025**] Towards facial micro-expression detection and classification using modified multimodal ensemble learning approach** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[173][**IEEE Transactions on Circuits and Systems for Video Technology, 2024**] Dep-former: Multimodal depression recognition based on facial expressions and audio features via emotional changes** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[174][**Expert Systems with Applications, 2024**] MSER: Multimodal speech emotion recognition using cross-attention with deep fusion** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[175][**IEEE JBHI, 2025**] Multimodal fusion of behavioral and physiological signals for enhanced emotion recognition via feature decoupling and knowledge transfer** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[176][**IEEE Transactions on Instrumentation and Measurement, 2024**] Deep learning-based automated emotion recognition using multimodal physiological signals and time-frequency methods** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[177][**MICCAI, 2025**] MedVLM-RL: Incentivizing medical reasoning capability of vision-language models (VLMs) via reinforcement learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[178][**arXiv, 2023**] DialogueLLM: Context and emotion knowledge-tuned large language models for emotion recognition in conversations** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

### 3.2 Medical Embodied Decision-Making

* ‚Äã**[179][**EBioMedicine, 2019**] Artificial intelligence to support clinical decision-making processes** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[180][**JAMA Surgery, 2020**] Artificial intelligence and surgical decision-making** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.2.1 Medical Workflow Modeling and Task Planning

* ‚Äã**[181][**BMC Oral Health, 2025**] Artificial intelligence and augmented reality for guided implant surgery planning: a proof of concept** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[182][**Health Systems, 2021**] Clinical pathway modelling: a literature review** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[183][**MICCAI, 2021**] Trans-SVNet: Accurate phase recognition from surgical videos via hybrid embedding aggregation transformer** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[184][**MICCAI, 2020**] TeCNO: Surgical phase recognition with multi-stage temporal convolutional networks** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[185][**IJCARS, 2022**] PATG: position-aware temporal graph networks for surgical phase recognition on laparoscopic videos** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[186][**IEEE-EMBS BHI, 2022**] Towards graph representation learning based surgical workflow anticipation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[187][**NeurIPS, 2023**] LLaVA-Med: Training a large language-and-vision assistant for biomedicine in one day** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.2.2 Medical Navigation Systems

* ‚Äã**[188][**Langenbeck's Archives of Surgery, 2013**] Navigation in surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[189][**International Journal of Nanomedicine, 2025**] Localized drug delivery in different gastrointestinal cancers: navigating challenges and advancing nanotechnological solutions** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[190][**ICSR, 2024**] Utilizing a social robot as a greeter at a children's hospital** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[191][**IPIN, 2015**] Navigating in large hospitals** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[192][**Neurosurgery, 1999**] BrainLAB VectorVision neuronavigation system: technology and clinical experiences in 131 cases** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[193][**Annals of Biomedical Engineering, 2021**] A wearable augmented reality navigation system for surgical telementoring based on Microsoft HoloLens** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[194][**IEEE Transactions on Automation Science and Engineering, 2025**] RL-USRegi: Autonomous ultrasound registration for radiation-free spinal surgical navigation using reinforcement learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[195][**IJCARS, 2024**] Autonomous navigation of catheters and guidewires in mechanical thrombectomy using inverse reinforcement learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[196][**AAAI, 2024**] NavGPT: Explicit reasoning in vision-and-language navigation with large language models** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[197][**ECCV, 2024**] NavGPT-2: Unleashing navigational reasoning capability for large vision-language models** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.2.3 Clinical Question-answering and Decision Support

* ‚Äã**[198][**Nature Medicine, 2025**] Toward expert-level medical question answering with large language models** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[199][**Artificial Intelligence in Medicine, 2023**] Medical visual question answering: A survey** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[200][**ACM Computing Surveys, 2022**] Biomedical question answering: a survey of approaches and challenges** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[201][**Nature Medicine, 2025**] Clinical implementation of an AI-based prediction model for decision support for patients undergoing colorectal cancer surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[202][**BMC Oral Health, 2025**] Artificial intelligence-based chatbot assistance in clinical decision-making for medically complex patients in oral surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[203][**Journal of Biology and Health Science, 2025**] Multimodal decision support system for improved diagnosis and healthcare decision making** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[204][**Computerized Medical Imaging and Graphics, 2025**] MedBlip: A multimodal method of medical question-answering based on fine-tuning large language model** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

### 3.3 Medical Embodied Action

* ‚Äã**[205][**Science Robotics, 2025**] Will your next surgeon be a robot? Autonomy and AI in robotic surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[206][**Annual Review of Control, Robotics, and Autonomous Systems, 2021**] Autonomy in surgical robotics** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.3.1 Medical Imitation-based Action

* ‚Äã**[207][**IEEE Transactions on Biomedical Engineering, 2025**] Imitation learning for path planning in cardiac percutaneous interventions** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[208][**arXiv, 2024**] Surgical robot transformer (SRT): Imitation learning for surgical tasks** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[209][**ICRA, 2021**] Intermittent visual servoing: Efficiently learning policies robust to instrument changes for high-precision surgical manipulation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[210][**arXiv, 2025**] SuFIA-BC: Generating high quality demonstration data for visuomotor policy learning in surgical subtasks** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[211][**IEEE Transactions on Biomedical Engineering, 2021**] Inverse reinforcement learning intra-operative path planning for steerable needle** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[212][**ICRA, 2020**] Collaborative robot-assisted endovascular catheterization with generative adversarial imitation learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[213][**IROS, 2024**] Towards a surgeon-in-the-loop ophthalmic robotic apprentice using reinforcement and imitation learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[214][**ICRA, 2022**] 3d perception based imitation learning under limited demonstration for laparoscope control in robotic surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.3.2 Medical Reinforcement-based Action

* ‚Äã**[215][**ACM Computing Surveys, 2021**] Reinforcement learning in healthcare: A survey** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[216][**RO-MAN, 2020**] Collaborative suturing: A reinforcement learning approach to automate hand-off task in suturing for surgical robots** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[217][**Journal of Machine Learning Research, 2023**] LapGym-an open source framework for reinforcement learning in robot-assisted laparoscopic surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[218][**ICMA, 2022**] Evaluation of an autonomous navigation method for vascular interventional surgery in virtual environment** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[219][**arXiv, 2024**] Surgical task automation using actor-critic frameworks and self-supervised imitation learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[220][**IEEE Transactions on Industrial Electronics, 2023**] CASOG: Conservative actor‚Äìcritic with smooth gradient for skill learning in robot-assisted intervention** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.3.3 Medical Large Model-Driven Action

* ‚Äã**[221][**Nature Reviews Electrical Engineering, 2025**] Innovating robot-assisted surgery through large vision models** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[222][**arXiv, 2024**] GP-VLS: A general-purpose vision language model for surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[223][**Artificial Intelligence Review, 2024**] Large language models in healthcare: from a systematic review on medical examinations to a comparative analysis on fundamentals of robotic surgery online test** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[224][**MICCAI, 2023**] SurgicalGPT: end-to-end language-vision GPT for visual question answering in surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[225][**ML4H, 2023**] Med-Flamingo: a multimodal medical few-shot learner** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[226][**arXiv, 2024**] RoboNurse-VLA: Robotic scrub nurse system based on vision-language-action model** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

### 3.4 Integrated Application Scenarios in Healthcare

#### 3.4.1 Surgical Robot

* ‚Äã**[227][**IEEE Robotics & Automation Magazine, 2021**] Accelerating surgical robotics research: A review of 10 years with the da Vinci research kit** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[228][**IJMRCAS, 2013**] Technical review of the da Vinci surgical telemanipulator** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[229][**Intuit. Surg., 2013**] da Vinci surgical system** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[230][**IJMRCAS, 2015**] A pneumatic laparoscope holder controlled by head movement** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[231][**Acta Neurochirurgica, 2016**] Minimally invasive transforaminal lumbar interbody fusion with the ROSA‚Ñ¢ spine robot and intraoperative flat-panel CT guidance** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[232][**Nature Biomedical Engineering, 2018**] First-in-human study of the safety and viability of intraocular robotic surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[233][**Clinical Orthopaedics and Related Research, 1998**] Primary and revision total hip replacement using the ROBODOC (R) system** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[234][**Surgical Innovation, 2024**] The use of the Symani Surgical System¬Æ in emergency hand trauma care** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[235][**Technology in Cancer Research & Treatment, 2010**] The CyberKnife¬Æ robotic radiosurgery system in 2010** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[236][**ICRA, 2019**] Robotic bronchoscopy drive mode of the Auris Monarch platform** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[237][**European Archives of Oto-Rhino-Laryngology, 2015**] Transoral robotic surgery (TORS) with the Medrobotics Flex‚Ñ¢ system: first surgical application on humans** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[238][**Journal of Neurosurgery, 2019**] Neuroendovascular-specific engineering modifications to the CorPath GRX robotic system** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.4.2 Intelligent Caregiving and Companion Robot

* ‚Äã**[239][**BMC Geriatrics, 2019**] The benefits of and barriers to using a social robot PARO in care settings: a scoping review** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[240][**European Journal of Pediatrics, 2022**] The pilot study of group robot intervention on pediatric inpatients and their caregivers, using 'new aibo'** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[241][**Envisioning the Future of Health Informatics and Digital Health, 2025**] The Pepper robot in healthcare: A scoping review** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[242][**Journal of Aging Research & Lifestyle, 2024**] ElliQ, an AI-driven social robot to alleviate loneliness: progress and lessons learned** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[243][**Journal of Engineering in Medicine, 2018**] Arash: A social robot buddy to support children with cancer in a hospital environment** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[244][**IJERPH, 2020**] Robotics utilization for healthcare digitization in global COVID-19 management** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[245][**RO-MAN, 2012**] Technical improvements of the Giraff telepresence robot based on users' evaluation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[246][**SIGGRAPH Emerging Technologies, 2011**] Telenoid: Tele-presence android for communication** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.4.3 Immersive Medical Education Platform

* ‚Äã**[247][**Handbook of Research on Educational Communications and Technology, 2008**] Cognitive task analysis** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[248][**PhD Thesis/Report, 2020**] The first experience of using the Body Interact simulation interactive training platform as a part of interns' attestation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[249][**Surgical Technology International, 2019**] Validation of the hip arthroscopy module of the VirtaMed virtual reality arthroscopy trainer** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[250][**Digital Diagnostics, 2024**] Possibilities for using the Vimedix 3.2 virtual simulator to train ultrasound specialists** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[251][**CVPR, 2022**] OSSO: Obtaining skeletal shape from outside** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[252][**JMLA, 2022**] 3D Organon VR Anatomy** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 3.4.4 Telecollaborative Diagnostic and Treatment System

* ‚Äã**[253][**Web, 2025**] Teladoc Health** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[254][**Nurse Leader, 2016**] Mercy virtual nursing: An innovative care delivery model** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[255][**Molecular Nutrition & Food Research, 2019**] Duck egg white‚Äìderived peptide VSEE (Val-Ser-Glu-Glu) regulates bone and lipid metabolisms by Wnt/Œ≤-catenin signaling pathway and intestinal microbiota** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[256][**Frontiers in Human Neuroscience, 2024**] A retrospective, observational study of real-world clinical data from the cognitive function development therapy program** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

## 4. Datasets and benchmark

### 4.1 Perception Datasets

#### 4.1.1 Organ‚ÄìInstrument Recognition Datasets

* ‚Äã**[257][**Scientific Data, 2022**] ISLES 2022: A multi-center magnetic resonance imaging stroke lesion segmentation dataset** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[258][**arXiv, 2023**] The state-of-the-art 3d anisotropic intracranial hemorrhage segmentation on non-contrast head ct: The instance challenge** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[259][**arXiv, 2023**] FairSeg: A large-scale medical image segmentation dataset for fairness learning using segment anything model with fair error-bound scaling** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[260][**Information Sciences, 2019**] Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[261][**Medical Image Analysis, 2022**] AtrialJSQnet: a new framework for joint segmentation and quantification of left atrium and scars incorporating spatial and shape information** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[262][**ATM'22 Challenge, 2023**] Multi-site, multi-domain airway tree modeling (ATM'22): A public benchmark for pulmonary airway segmentation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[263][**IPTA, 2020**] SegTHOR: Segmentation of thoracic organs at risk in CT images** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[264][**arXiv, 2025**] Tumor detection, segmentation and classification challenge on automated 3d breast ultrasound: The TDSC-ABUS challenge** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[265][**Web, 2010**] 3D image reconstruction for comparison of algorithm database (3D-IRCADb-01)** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[266][**Medical Image Analysis, 2021**] VerSe: a vertebrae labelling and segmentation benchmark for multi-detector CT images** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[267][**Scientific Data, 2024**] Lumbar spine segmentation in MR images: a dataset and a public benchmark** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[268][**arXiv, 2021**] CTSpine1K: A large-scale dataset for spinal vertebrae segmentation in computed tomography** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[269][**IJCARS, 2021**] Deep learning to segment pelvic bones: large-scale CT datasets and baseline models** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[270][**arXiv, 2016**] Skin lesion analysis toward melanoma detection: A challenge at the international symposium on biomedical imaging (ISBI) 2016** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[271][**Expert Systems with Applications, 2015**] MED-NODE: A computer-assisted melanoma diagnosis system using non-dermoscopic images** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[272][**Dermoscopy Image Analysis, 2015**] PH2: A public database for the analysis of dermoscopic images** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[273][**Data in Brief, 2020**] PAD-UFES-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[274][**bioRxiv, 2022**] A web-scraped skin image database of monkeypox, chickenpox, smallpox, cowpox, and measles** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[275][**Radiology: Artificial Intelligence, 2023**] TotalSegmentator: robust segmentation of 104 anatomic structures in CT images** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[276][**Medical Image Analysis, 2025**] The ULS23 challenge: A baseline model and benchmark dataset for 3D universal lesion segmentation in computed tomography** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[277][**Scientific Data, 2022**] A whole-body FDG-PET/CT dataset with manually annotated tumor lesions** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[278][**arXiv, 2019**] 2017 robotic instrument segmentation challenge** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[279][**IROS, 2020**] LC-GAN: Image-to-image translation based on generative adversarial network for endoscopic images** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[280][**arXiv, 2024**] SegStrong-C: Segmenting surgical tools robustly on non-adversarial generated corruptions‚Äìan Endovis' 24 challenge** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[281][**IEEE Transactions on Medical Imaging, 2016**] EndoNet: a deep architecture for recognition tasks on laparoscopic videos** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[282][**CVPR, 2025**] CholecTrack20: A multi-perspective tracking dataset for surgical tools** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[283][**arXiv, 2023**] The EndoScapes dataset for surgical scene segmentation, object detection, and critical view of safety assessment** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[284][**arXiv, 2020**] m2caiSeg: Semantic segmentation of laparoscopic images using convolutional neural networks** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[285][**arXiv, 2021**] FetReg: Placental vessel segmentation and registration in fetoscopy challenge dataset** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 4.1.2 Medical Scene Modeling Datasets

* ‚Äã**[286][**CVPR, 2025**] MM-OR: A large multimodal operating room dataset for semantic understanding of high-intensity surgical environments** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[287][**IEEE Transactions on Biomedical Engineering, 2016**] See it with your own eyes: Markerless mobile augmented reality for radiation awareness in the hybrid room** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[288][**Engineering Applications of Artificial Intelligence, 2023**] Object detection in hospital facilities: A comprehensive dataset and performance evaluation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[289][**Data in Brief, 2018**] MCIndoor20000: A fully-labeled image dataset to advance indoor objects detection** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[290][**Data in Brief, 2020**] MyNursingHome: A fully-labelled image dataset for indoor object classification** [paper](https://www.google.com/url?sa=E&q=placeholder)

#### 4.1.3 Clinical Action and Pose Estimation Datasets

* ‚Äã**[291][**arXiv, 2018**] MVOR: A multi-view RGB-D operating room dataset for 2d and 3d human pose estimation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[292][**IEEE Journal of Translational Engineering in Health and Medicine, 2018**] Patient-specific pose estimation in clinical environments** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[293][**Algorithms, 2024**] MMD-MSD: A multimodal multisensory dataset in support of research and technology development for musculoskeletal disorders** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[294][**MICCAI Workshops, 2025**] SurgTrack: CAD-free 3D tracking of real-world surgical instruments** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 4.1.4 Multimodal Affective Perception Datasets

* ‚Äã**[295][**arXiv, 2025**] Advancing face-to-face emotion communication: A multimodal dataset (AffEc)** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[296][**NeurIPS, 2024**] Emotion-LLaMA: Multimodal emotion recognition and reasoning with instruction tuning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[297][**Scientific Data, 2024**] A multimodal dataset for mixed emotion recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[298][**IEEE Transactions on Affective Computing, 2024**] SEED-VII: A multimodal dataset of six basic emotions with continuous labels for emotion recognition** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[299][**IEEE Transactions on Affective Computing, 2016**] ASCERTAIN: Emotion and personality recognition using commercial sensors** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[300][**IEEE JBHI, 2017**] DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

### 4.2 Decision-Making Datasets

#### 4.2.1 Surgical Workflow Annotation Datasets

* ‚Äã**[301][**ECCV, 2024**] OphNet: A large-scale video benchmark for ophthalmic surgical workflow understanding** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[302][**MICCAI, 2022**] AutoLaparo: A new dataset of integrated multi-tasks for image-guided surgical automation in laparoscopic hysterectomy** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[303][**Scientific Data, 2025**] LapEx: A new multimodal dataset for context recognition and practice assessment in laparoscopic surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[304][**Computer Methods and Programs in Biomedicine, 2021**] Micro-surgical anastomose workflow recognition challenge report** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 4.2.2 Medical Navigation Datasets

* ‚Äã**[305][**IEEE Sensors Journal, 2025**] A portable 6D surgical instrument magnetic localization system with dynamic error correction** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[306][**Scientific Data, 2024**] Head model dataset for mixed reality navigation in neurosurgical interventions for intracranial lesions** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[307][**CVPR, 2018**] Gibson Env: Real-world perception for embodied agents** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[308][**arXiv, 2021**] Habitat-Matterport 3D dataset (HM3D): 1000 large-scale 3d environments for embodied AI** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 4.2.3 Medical Question Answering Datasets

* ‚Äã**[309][**IJCARS, 2024**] Advancing surgical VQA with scene graph knowledge** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[310][**arXiv, 2024**] ErVQA: A dataset to benchmark the readiness of large vision language models in hospital environments** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[311][**arXiv, 2025**] MedReason: Eliciting factual medical reasoning steps in LLMs via knowledge graphs** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[312][**EMNLP, 2025**] ReasonMed: A 370k multi-agent generated dataset for advancing medical reasoning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[313][**arXiv, 2025**] ORQA: A benchmark and foundation model for holistic operating room modeling** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[314][**arXiv, 2024**] LLaVA-Surg: towards multimodal surgical assistant via structured surgical video learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

### 4.3 Action Datasets

* ‚Äã**[315][**MICCAI Workshop, 2014**] JHU-ISI gesture and skill assessment working set (JIGSAWS): A surgical activity dataset for human motion modeling** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[316][**arXiv, 2023**] The EndoScapes dataset for surgical scene segmentation, object detection, and critical view of safety assessment** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[317][**IJCARS, 2024**] Challenges in multi-centric generalization: phase and step recognition in Roux-en-Y gastric bypass surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[318][**arXiv, 2025**] Surgical visual understanding (SurgVU) dataset** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[319][**arXiv, 2016**] The TUM LapChole dataset for the M2CAI 2016 workflow challenge** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[320][**arXiv, 2024**] General surgery vision transformer: A video pre-trained foundation model for general surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[321][**arXiv, 2022**] MITI: SLAM benchmark for laparoscopic surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[322][**ACM Multimedia, 2025**] COPESD: A multi-level surgical motion dataset for training large vision-language models to co-pilot endoscopic submucosal dissection** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

### 4.4 Simulation Platforms and Synthetic Datasets

#### 4.4.1 Surgical Simulation Platforms

* ‚Äã**[323][**IROS, 2021**] SurRoL: An open-source reinforcement learning centered and dVRK compatible platform for surgical robot learning** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[324][**ICRA, 2024**] Orbit-Surgical: An open-simulation framework for learning surgical augmented dexterity** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[325][**ICRA, 2024**] Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[326][**Journal of Machine Learning Research, 2023**] LapGym-an open source framework for reinforcement learning in robot-assisted laparoscopic surgery** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[327][**arXiv, 2025**] SonoGym: High performance simulation for challenging surgical tasks with robotic ultrasound** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

#### 4.4.2 Synthetic Datasets

* ‚Äã**[328][**arXiv, 2023**] SynFundus-1M: a high-quality million-scale synthetic fundus images dataset with fifteen types of annotation** [paper](https://www.google.com/url?sa=E&q=placeholder)
  ‚Äã
* ‚Äã**[329][**Scientific Data, 2023**] A large-scale synthetic pathological dataset for deep learning-enabled segmentation of breast cancer** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[330][**Intelligence-Based Medicine, 2020**] Synthea‚Ñ¢ novel coronavirus (COVID-19) model and synthetic data set** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã
* ‚Äã**[331][**Electronics, 2022**] The ‚Äúcoherent data set‚Äù: combining patient data and imaging in a comprehensive, synthetic health record** [paper](https://www.google.com/url?sa=E&q=placeholder)
  
  ‚Äã

