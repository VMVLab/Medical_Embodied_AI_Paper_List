<div align=center>
<img src="img\logo.png" width="180px">
</div>
<h2 align="center"><a src="paper\medical_embodied_ai.pdf"> Towards Next-Generation Healthcare: A Survey of Medical Embodied AI for Perception, Decision-Making, and Action </a></h2>
<h5 align="center"> If you like our project, please give us a star ‚≠ê on GitHub for the latest update.</h5>

## üè† About

Foundation models have demonstrated impressive performance in enhancing  healthcare efficiency. However, their limited ability to perceive and interact  with the physical world significantly constrains their utility in real-world clinical  workflows. Recently, embodied artificial intelligence (AI) provides a promising  physical-interactive paradigm for intelligent healthcare by integrating percep tion, decision-making, and action within a closed-loop system. Nevertheless, the  exploration of embodied AI for healthcare is still in its infancy. To support these  advances, this review systematically surveys the key components of embodied AI,  focusing on the integration of perception, decision-making, and action. Addition ally, we present a comprehensive overview of representative medical applications,  relevant datasets, major challenges in clinical practice, and further discuss the  key directions for future research in this emerging field. The associated project  can be found at XXXX.

<div align="center">

### [1. Introduction](#1-introduction) | [2. Embodied AI](#2-embodied-ai) |  [3. Embodied AI in Medicine](#3-embodied-ai-in-medicine)

### [4. Datasets and benchmark](#4-datasets-and-benchmark) | [5. Challenges and Outlook](#5-challenges-and-outlook) | [6. Conclusion](#6-conclusion)

</div>

## 1. Introduction

* [1][**Nature medicine, 2024**] Artificial intelligence in surgery [paper](https://www.nature.com/articles/s41591-024-02970-3)

## 2. Embodied AI

### 2.1 Embodied Perception

#### 2.1.1 Object Perception
* [2][**IEEE Transactions on Cognitive and Developmental Systems, 2020**] Robot Multimodal Object Perception and Recognition: Synthetic Maturation of Sensorimotor Learning in Embodied Systems [paper](#)
* [3][**IEEE Transactions on Robotics, 2025**] Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference [paper](#)

#### 2.1.2 Scene Perception
* [4][**CVPR, 2025**] Embodied Scene Understanding for Vision Language Models via MetaVQA [paper](#)
* [5][**ECCV, 2024**] Embodied Understanding of Driving Scenarios [paper](#)
* [6][**ECCV, 2024**] SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding [paper](#)

#### 2.1.3 Behavior Perception
* [7][**Expert Systems with Applications, 2024**] Human activity recognition with smartphone-integrated sensors: A survey [paper](#)
* [8][**AI Review, 2024**] A survey of video-based human action recognition in team sports [paper](#)
* [9][**IEEE Transactions on Information Forensics and Security, 2025**] Collaboratively Self-Supervised Video Representation Learning for Action Recognition [paper](#)

#### 2.1.4 Expression Perception
* [10][**IEEE Transactions on Affective Computing, 2020**] Deep facial expression recognition: A survey [paper](#)
* [11][**IEEE Transactions on Affective Computing, 2022**] Deep learning for micro-expression recognition: A survey [paper](#)
* [12][**Proceedings of the IEEE, 2023**] Facial Micro-Expressions: An Overview [paper](#)

### 2.2 Embodied Decision-Making

#### 2.2.1 Task Planning
* [13][**ICRA, 2024**] Anticipate & Act: Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments [paper](#)
* [14][**ICRA, 2025**] CaStL: Constraints as Specifications Through LLM Translation for Long-Horizon Task and Motion Planning [paper](#)

#### 2.2.2 Embodied Navigation
* [15][**IEEE Transactions on Automation Science and Engineering, 2024**] A Survey of Object Goal Navigation [paper](#)
* [16][**TPAMI, 2025**] GaussNav: Gaussian Splatting for Visual Navigation [paper](#)
* [17][**TPAMI, 2025**] Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments [paper](#)

#### 2.2.3 Embodied Question Answering (EQA)
* [18][**WACV, 2025**] Scene-LLM: Extending Language Model for 3D Visual Reasoning [paper](#)

### 2.3 Embodied Action

#### 2.3.1 Imitation Learning-Based Action
* [19][**NeurIPS, 2024**] Is behavior cloning all you need? understanding horizon in imitation learning [paper](#)
* [20][**IEEE Robotics and Automation Letters, 2025**] Stable-BC: Controlling Covariate Shift With Stable Behavior Cloning [paper](#)
* [21][**ICRA, 2025**] Egomimic: Scaling imitation learning via egocentric video [paper](#)
* [22][**IEEE Transactions on Industrial Electronics, 2025**] Deep Multimodal Imitation Learning-Based Framework for Robot-Assisted Medical Examination [paper](#)

#### 2.3.2 Reinforcement Learning-Based Action
* [23][**IEEE Transactions on Neural Networks and Learning Systems, 2022**] Deep reinforcement learning: A survey [paper](#)
* [24][**AAAI, 2025**] Deep reinforcement learning for robotics: A survey of real-world successes [paper](#)
* [25][**arXiv, 2017**] Proximal policy optimization algorithms [paper](#)
* [26][**ICML, 2018**] Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor [paper](#)

#### 2.3.3 Large Model-Driven Action
* [27][**arXiv, 2023**] GPT-4 Technical Report [paper](#)
* [28][**arXiv, 2023**] PaLM-E: An Embodied Multimodal Language Model [paper](#)
* [29][**arXiv, 2025**] MedVLM-R1: Incentivizing medical reasoning capability of vision-language models via reinforcement learning [paper](#)

## 3. Embodied AI in Medicine

### 3.1 Medical Embodied Perception

#### 3.1.1 Medical Instrument and Organ Recognition
* [30][**MICCAI, 2019**] Using 3D convolutional neural networks to learn spatiotemporal features for automatic surgical gesture recognition in video [paper](#)
* [31][**IROS, 2022**] Recognition and prediction of surgical gestures and trajectories using transformer models in robot-assisted surgery [paper](#)

#### 3.1.2 Surgical and Clinical Environment Perception and Modeling
* [32][**IROS, 2024**] MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements [paper](#)
* [33][**TPAMI, 2022**] Learning View-Based Graph Convolutional Network for Multi-View 3D Shape Analysis [paper](#)

#### 3.1.3 Medical Operation Behavior Detection
* [34][**IEEE Transactions on Medical Imaging, 2022**] Gesture recognition in robotic surgery with multimodal attention [paper](#)

#### 3.1.4 Emotional Interaction Understanding
* [35][**Pattern Recognition, 2025**] Multimodal latent emotion recognition from micro-expression and physiological signal [paper](#)
* [36][**Information Sciences, 2025**] Towards facial micro-expression detection and classification using modified multimodal ensemble learning approach [paper](#)

### 3.2 Medical Embodied Decision-Making

#### 3.2.1 Medical Workflow Modeling and Task Planning
* [37][**EBioMedicine, 2019**] Artificial intelligence to support clinical decision-making processes [paper](#)

#### 3.2.2 Medical Navigation Systems
* [38][**IEEE Transactions on Robotics, 2023**] Autonomous Navigation for Robot-Assisted Intraluminal and Endovascular Procedures: A Systematic Review [paper](#)
* [39][**IEEE Transactions on Automation Science and Engineering, 2025**] Sim2Real Learning With Domain Randomization for Autonomous Guidewire Navigation in Robotic-Assisted Endovascular Procedures [paper](#)

#### 3.2.3 Clinical Question-answering and Decision Support
* [40][**Nature Medicine, 2025**] Toward expert-level medical question answering with large language models [paper](#)

### 3.3 Medical Embodied Action

#### 3.3.1 Medical Imitation-based Action
* [41][**IEEE Transactions on Industrial Electronics, 2025**] Deep Multimodal Imitation Learning-Based Framework for Robot-Assisted Medical Examination [paper](#)

#### 3.3.2 Medical Reinforcement-based Action
* [42][**AAAI, 2025**] Deep reinforcement learning for robotics: A survey of real-world successes [paper](#)

#### 3.3.3 Medical Large Model-Driven Action
* [43][**arXiv, 2023**] PaLM-E: An Embodied Multimodal Language Model [paper](#)

### 3.4 Integrated Application Scenarios in Healthcare

#### 3.4.1 Surgical Robot
* [44][**Science Robotics, 2025**] Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery [paper](#)
* [45][**Proceedings of the IEEE, 2022**] Concepts and Trends in Autonomy for Robot-Assisted Surgery [paper](#)
* [46][**Nature Reviews Bioengineering, 2025**] Robotic surgery [paper](#)

#### 3.4.2 Intelligent Caregiving and Companion Robot
* [47][**Journal of Medical Internet Research, 2024**] Embodied Conversational Agents for Chronic Diseases: Scoping Review [paper](#)

#### 3.4.3 Immersive Medical Education Platform
* [48][**i-CREATe, 2024**] Virtual Co-Embodiment Rehabilitation: An Innovative Method Integrating Virtual Co-Embodiment and Action Observation Therapy in Virtual Reality Rehabilitation [paper](#)

#### 3.4.4 Telecollaborative Diagnostic and Treatment System
* [49][**Autonomous Robots, 2024**] Boosting the hospital by integrating mobile robotic assistance systems: a comprehensive classification of the risks to be addressed [paper](#)
* [50][**ICSR+AI, 2025**] Interaction Matters When It Comes to Hand Disinfection Using Robots at Hospitals [paper](#)

## 4. Datasets and benchmark

### 4.1 Perception Datasets

#### 4.1.1 Organ‚ÄìInstrument Recognition Datasets
* [51][**Scientific Data, 2025**] EMG dataset for gesture recognition with arm translation [paper](#)

#### 4.1.2 Medical Scene Modeling Datasets
* [52][**WACV, 2024**] U3ds3: Unsupervised 3D semantic scene segmentation [paper](#)

#### 4.1.3 Clinical Action and Pose Estimation Datasets
* [53][**Scientific Data, 2025**] EMG dataset for gesture recognition with arm translation [paper](#)

#### 4.1.4 Multimodal Affective Perception Datasets
* [54][**Proceedings of the IEEE, 2023**] Facial Micro-Expressions: An Overview [paper](#)

### 4.2 Decision-Making Datasets
* [55][**Nature Medicine, 2025**] Toward expert-level medical question answering with large language models [paper](#)

#### 4.2.1 Surgical Workflow Annotation Datasets

#### 4.2.2 Medical Navigation Datasets

#### 4.2.3 Medical Question Answering Datasets

### 4.3 Action Datasets
* [56][**ICRA, 2025**] Egomimic: Scaling imitation learning via egocentric video [paper](#)

### 4.4 Simulation Platforms and Synthetic Datasets

#### 4.4.1 Surgical Simulation Platforms
* [57][**Science Robotics, 2025**] Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery [paper](#)

#### 4.4.2 Synthetic Datasets
* [58][**arXiv, 2025**] DexScale: Sim2Real Generalizable Robot Control Data Scaling [paper](#)

## 5. Challenges and Outlook

### 5.1 Insufficient Training Data and Perception Discrepancy
* [59][**Information Fusion, 2025**] From screens to scenes: A survey of embodied AI in healthcare [paper](#)

### 5.2 Semantic Ambiguity and Multimodal Knowledge Fusion  Difficulties
* [60][**Information Fusion, 2025**] Advancements in perception system with multi-sensor fusion for embodied agents [paper](#)

### 5.3 Medical Reasoning Complexity and Uncertainty Modeling
* [61][**arXiv, 2025**] MedVLM-R1: Incentivizing medical reasoning capability of vision-language models via reinforcement learning [paper](#)

### 5.4 Lack of Mechanisms for Decision Pathway Generation and  Validation
* [62][**arXiv, 2025**] Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning [paper](#)

### 5.5 Error Sensitivity in High-Precision Action Control
* [63][**ICML, 2024**] Position: a call for embodied AI [paper](#)

### 5.6 Lack of General-Purpose Medical Simulation Platforms
* [64][**Science Robotics, 2025**] Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery [paper](#)

## 6. Conclusion
* [65][**ACM Computing Surveys, 2025**] Embodied intelligence: A synergy of morphology, action, perception and learning [paper](#)

