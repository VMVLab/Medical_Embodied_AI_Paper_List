<a id="top"></a>

<div align=center>
<img src="img\logo.png" width="180px">
</div>
<h2 align="center"><a href="paper/medical_embodied_ai.pdf"> Towards Next-Generation Healthcare: A Survey of Medical Embodied AI for Perception, Decision-Making, and Action </a></h2>
<div align="center">

[![arXiv](https://img.shields.io/badge/arXiv-XXX-orange)](XXX)
![Type-Survey](https://img.shields.io/badge/Type-Survey-blue)
![Topic-Medical%20Embodied%20AI](https://img.shields.io/badge/Topic-Medical%20Embodied%20AI-purple)
[![](https://img.shields.io/badge/Project-%F0%9F%9A%80-pink)](https://github.com/VMVLab/Medical_Embodied_AI_Paper_List)

</div>

> [!NOTE]
> üåü If this project is helpful to you, please click on a Star to support usÔºÅ

---

## üè† About

Foundation models have demonstrated impressive performance in enhancing  healthcare efficiency. However, their limited ability to perceive and interact  with the physical world significantly constrains their utility in real-world clinical  workflows. Recently, embodied artificial intelligence (AI) provides a promising  physical-interactive paradigm for intelligent healthcare by integrating percep tion, decision-making, and action within a closed-loop system. Nevertheless, the  exploration of embodied AI for healthcare is still in its infancy. To support these  advances, this review systematically surveys the key components of embodied AI,  focusing on the integration of perception, decision-making, and action. Addition ally, we present a comprehensive overview of representative medical applications,  relevant datasets, major challenges in clinical practice, and further discuss the  key directions for future research in this emerging field. The associated project  can be found at XXXX.

<div align="center">

### [üìñ 1. Introduction](#1-introduction) | [ü§ñ 2. Embodied AI](#2-embodied-ai)

### [üè• 3. Embodied AI in Medicine](#3-embodied-ai-in-medicine) | [üìä 4. Datasets and benchmark](#4-datasets-and-benchmark)

</div>

## üìñ 1. Introduction

<div align=center>
<img src="img\Fig1.jpg">
</div>

Foundations of embodied AI. a, Publication volume, temporal trends over the past decade,  and representative keywords related to embodied intelligence. The statistics are obtained from Google  Scholar using ‚Äúembodied AI‚Äù as the search query. b, The four developmental stages of embodied intel ligence, namely the Conceptual Germination Stage, Paradigm Shift Stage, Learning-Driven Stage,  and Large Model Empowered Stage. c, A comparison between disembodied intelligence and embodied  intelligence. Unlike its disembodied counterpart, embodied intelligence is distinguished by its inher ent ability to interact with the environment. d, Core components of embodied intelligence. At the  macroscopic level, it consists of agents and their environments; at the technical level, it encompasses  embodied perception, embodied decision-making, and embodied action.

<div align="center">
<a href="#top">‚¨Ü Back to top</a>
</div>

## ü§ñ 2. Embodied AI

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] Neural Brain: A Neuroscience-inspired Framework for Embodied Agents [paper](https://arxiv.org/abs/2505.07634)
* [**ACM Computing Surveys, 2025**] Embodied Intelligence: A Synergy of Morphology, Action, Perception and Learning [paper](https://dl.acm.org/doi/10.1145/3717059)
* [**arXiv, 2025**] EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling [paper](https://arxiv.org/abs/2507.05198)
* [**arXiv, 2025**] Bridging the Sim2Real Gap: Vision Encoder Pre-Training for Visuomotor Policy Transfer [paper](https://arxiv.org/abs/2501.16389)
* [**ICML, 2025**] DexScale: Automating Data Scaling for Sim2Real Generalizable Robot Control [paper](https://openreview.net/forum?id=AVVXX0erKT)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2024**] DexSim2Real^2: Building Explicit World Model for Precise Articulated Object Dexterous Manipulation [paper](https://arxiv.org/abs/2409.08750)
* [**ICML, 2024**] Position: A Call for Embodied AI [paper](https://proceedings.mlr.press/v235/paolo24a.html)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Mind, 1950**] Computing machinery and intelligence [paper](https://doi.org/10.1093/mind/LIX.236.433)

### 2.1 Embodied Perception

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Information Fusion, 2025**] Tactile data generation and applications based on visuo-tactile sensors: A review [paper](https://doi.org/10.1016/j.inffus.2025.103162)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**CVPR, 2024**] Evidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception [paper](https://openaccess.thecvf.com/content/CVPR2024/html/Fan_Evidential_Active_Recognition_Intelligent_and_Prudent_Open-World_Embodied_Perception_CVPR_2024_paper.html)
* [**CVPR, 2024**] EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI [paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.html)
* [**Information Fusion, 2024**] Advancements in perception system with multi-sensor fusion for embodied agents [paper](https://doi.org/10.1016/j.inffus.2024.102859)

#### 2.1.1 Object Perception

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE Transactions on Robotics, 2025**] Predictive visuo-tactile interactive perception framework for object properties inference [paper](https://ieeexplore.ieee.org/document/10847911/)
* [**arXiv, 2025**] DINOv3 [paper](https://arxiv.org/abs/2508.10104)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2024**] SAM 2: Segment Anything in Images and Videos [paper](https://arxiv.org/abs/2408.00714)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**ICCV, 2023**] Segment Anything [paper](https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html)
* [**arXiv, 2023**] DINOv2: Learning Robust Visual Features without Supervision [paper](https://arxiv.org/abs/2304.07193)
* [**ECCV, 2022**] Open Vocabulary Object Detection with Pseudo Bounding-Box Labels [paper](https://doi.org/10.1007/978-3-031-20080-9_16)
* [**ICCV, 2021**] Emerging Properties in Self-Supervised Vision Transformers [paper](https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)
* [**CVPR, 2021**] Open-Vocabulary Object Detection Using Captions [paper](https://openaccess.thecvf.com/content/CVPR2021/html/Zareian_Open-Vocabulary_Object_Detection_Using_Captions_CVPR_2021_paper.html)
* [**IEEE Transactions on Cognitive and Developmental Systems, 2020**] Robot multimodal object perception and recognition: Synthetic maturation of sensorimotor learning in embodied systems [paper](https://doi.org/10.1109/TCDS.2020.2965985)
* [**NeurIPS, 2017**] Attention Is All You Need [paper](https://arxiv.org/abs/1706.03762)
* [**CVPR, 2016**] Deep Residual Learning for Image Recognition [paper](https://ieeexplore.ieee.org/document/7780459/)
* [**CVPR, 2016**] You Only Look Once: Unified, Real-Time Object Detection [paper](https://arxiv.org/abs/1506.02640)
* [**NeurIPS, 2015**] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks [paper](https://arxiv.org/abs/1506.01497)
* [**MICCAI, 2015**] U-Net: Convolutional Networks for Biomedical Image Segmentation [paper](https://doi.org/10.1007/978-3-319-24574-4_28)
* [**ACM Transactions on Graphics, 2015**] SMPL: A Skinned Multi-Person Linear Model [paper](https://doi.org/10.1145/2816795.2818013)
* [**arXiv, 2014**] Very Deep Convolutional Networks for Large-Scale Image Recognition [paper](https://arxiv.org/abs/1409.1556)
* [**ACCV, 2014**] 3D Human Pose Estimation from Monocular Images with Deep Convolutional Neural Network [paper](https://doi.org/10.1007/978-3-319-16808-1_23)
* [**NeurIPS, 2012**] ImageNet Classification with Deep Convolutional Neural Networks [paper](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)
* [**Proceedings of the IEEE, 1998**] Gradient-based learning applied to document recognition [paper](https://doi.org/10.1109/5.726791)

#### 2.1.2 Scene Perception

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**CVPR, 2025**] Embodied Scene Understanding for Vision Language Models via MetaVQA [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Embodied_Scene_Understanding_for_Vision_Language_Models_via_MetaVQA_CVPR_2025_paper.html)
* [**WACV, 2025**] Scene-LLM: Extending Language Model for 3D Visual Reasoning [paper](https://openaccess.thecvf.com/content/WACV2025/papers/Fu_Scene-LLM_Extending_Language_Model_for_3D_Visual_Reasoning_WACV_2025_paper.pdf)
* [**arXiv, 2025**] PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era [paper](https://arxiv.org/abs/2509.12989)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**ECCV, 2024**] Embodied Understanding of Driving Scenarios [paper](https://link.springer.com/chapter/10.1007/978-3-031-73033-7_8)
* [**ECCV, 2024**] SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding [paper](https://link.springer.com/chapter/10.1007/978-3-031-72673-6_16)
* [**WACV, 2024**] U3DS3: Unsupervised 3D Semantic Scene Segmentation [paper](https://openaccess.thecvf.com/content/WACV2024/html/Liu_U3DS3_Unsupervised_3D_Semantic_Scene_Segmentation_WACV_2024_paper.html)
* [**IEEE TPAMI, 2024**] ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments [paper](https://ieeexplore.ieee.org/abstract/document/10495141/)
* [**ICRA, 2024**] RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation [paper](https://arxiv.org/abs/2405.05792)
* [**IROS, 2024**] OmniNxt: A Fully Open-Source and Compact Aerial Robot with Omnidirectional Visual Perception [paper](https://ieeexplore.ieee.org/document/10802134/)
* [**ISPRS Journal of Photogrammetry and Remote Sensing, 2024**] Few-shot remote sensing image scene classification: Recent advances, new baselines, and future trends [paper](https://www.sciencedirect.com/science/article/pii/S0924271624000509)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**IEEE TPAMI, 2022**] Learning View-Based Graph Convolutional Network for Multi-View 3D Shape Analysis [paper](https://doi.org/10.1109/TPAMI.2022.3221785)

#### 2.1.3 Behavior Perception

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Pattern Recognition, 2025**] SAM-Net: Semantic-assisted multimodal network for action recognition in RGB-D videos [paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320325003851)
* [**IEEE Transactions on Information Forensics and Security, 2025**] Collaboratively self-supervised video representation learning for action recognition [paper](https://arxiv.org/abs/2401.07584)
* [**ICRA, 2025**] CaStL: Constraints as specifications through LLM translation for long-horizon task and motion planning [paper](https://doi.org/10.1109/ICRA55743.2025.11127555)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Expert Systems with Applications, 2024**] Human activity recognition with smartphone-integrated sensors: A survey [paper](https://doi.org/10.1016/j.eswa.2024.123143)
* [**Artificial Intelligence Review, 2024**] A survey of video-based human action recognition in team sports [paper](https://doi.org/10.1007/s10462-024-10934-9)
* [**Expert Systems with Applications, 2024**] A new framework for deep learning video-based human action recognition on the edge [paper](https://doi.org/10.1016/j.eswa.2023.122220)
* [**CVPR, 2024**] BlockGCN: Redefine topology awareness for skeleton-based action recognition [paper](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_BlockGCN_Redefine_Topology_Awareness_for_Skeleton-Based_Action_Recognition_CVPR_2024_paper.html)
* [**IEEE Transactions on Image Processing, 2024**] Learnable feature augmentation framework for temporal action localization [paper](https://doi.org/10.1109/TIP.2024.3413599)
* [**ICRA, 2024**] Anticipate & Act: Integrating LLMs and classical planning for efficient task execution in household environments [paper](https://arxiv.org/abs/2502.02066)

#### 2.1.4 Expression Perception

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects [paper](https://arxiv.org/abs/2505.20511)
* [**IEEE Transactions on Affective Computing, 2025**] MER-CLIP: AU-Guided Vision-Language Alignment for Micro-Expression Recognition [paper](https://arxiv.org/abs/2505.05937)
* [**Pattern Recognition, 2025**] Context Transformer with Multiscale Fusion for Robust Facial Emotion Recognition [paper](https://www.sciencedirect.com/science/article/pii/S0031320325003802)
* [**IEEE Transactions on Consumer Electronics, 2025**] Meta-Transfer Learning-Based Cross-Domain Gesture Recognition Using WiFi Channel State Information [paper](https://ieeexplore.ieee.org/document/10934063/)
* [**Scientific Data, 2025**] EMG Dataset for Gesture Recognition with Arm Translation [paper](https://www.nature.com/articles/s41597-024-04296-8)
* [**CVPR, 2025**] Uncertain Multimodal Intention and Emotion Understanding in the Wild [paper](https://ieeexplore.ieee.org/document/11092537/)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE TPAMI, 2024**] Prompt Tuning of Deep Neural Networks for Speaker-Adaptive Visual Speech Recognition [paper](https://ieeexplore.ieee.org/document/10726873/)
* [**IEEE Transactions on Instrumentation and Measurement, 2024**] Deep learning-based automated emotion recognition using multimodal physiological signals and time-frequency methods [paper](https://ieeexplore.ieee.org/document/10579770/)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Proceedings of the IEEE, 2023**] Facial Micro-Expressions: An Overview [paper](https://ieeexplore.ieee.org/document/10144523/)
* [**IEEE Transactions on Instrumentation and Measurement, 2023**] Understanding Deep Learning Techniques for Recognition of Human Emotions Using Facial Expressions: A Comprehensive Survey [paper](https://ieeexplore.ieee.org/document/10041168/)

### 2.2 Embodied Decision-Making

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges and Perspectives [paper](https://arxiv.org/abs/2503.13415)

#### 2.2.1 Task Planning

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**ICRA, 2025**] Guiding long-horizon task and motion planning with vision language models [paper](https://doi.org/10.48550/arXiv.2410.02193)
* [**ICRA, 2025**] DELTA: Decomposed efficient long-term robot task planning using large language models [paper](https://ieeexplore.ieee.org/document/11127838/)
* [**ICRA, 2025**] Fast and accurate task planning using neuro-symbolic language models and multi-level goal decomposition [paper](https://ieeexplore.ieee.org/document/11127617/)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE/ASME Transactions on Mechatronics, 2024**] A survey of optimization-based task and motion planning: From classical to learning approaches [paper](https://doi.org/10.1109/TMECH.2024.3452509)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**ACM Computing Surveys, 2023**] Recent trends in task and motion planning for robotics: A survey [paper](https://doi.org/10.1145/3583136)
* [**IROS, 2021**] Learning symbolic operators for task and motion planning [paper](https://doi.org/10.1109/IROS51168.2021.9635941)
* [**Journal of Artificial Intelligence Research, 2003**] PDDL2.1: An extension to PDDL for expressing temporal planning domains [paper](https://doi.org/10.1613/jair.1129)

#### 2.2.2 Embodied Navigation

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Science China Information Sciences, 2025**] Embodied navigation [paper](https://link.springer.com/article/10.1007/s11432-024-4303-8)
* [**CVPR, 2025**] MNE-SLAM: Multi-Agent Neural SLAM for Mobile Robots [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Deng_MNE-SLAM_Multi-Agent_Neural_SLAM_for_Mobile_Robots_CVPR_2025_paper.html)
* [**Information Sciences, 2025**] MAHACO: Multi-Algorithm Hybrid Ant Colony Optimizer for 3D Path Planning of a Group of UAVs [paper](https://doi.org/10.1016/j.ins.2024.121714)
* [**AAAI, 2025**] NaviFormer: A Spatio-Temporal Context-Aware Transformer for Object Navigation [paper](https://doi.org/10.1609/aaai.v39i14.33612)
* [**CVPR, 2025**] Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Song_Towards_Long-Horizon_Vision-Language_Navigation_Platform_Benchmark_and_Method_CVPR_2025_paper.html)
* [**IEEE TPAMI, 2025**] GaussNav: Gaussian Splatting for Visual Navigation [paper](https://ieeexplore.ieee.org/document/10870413/)
* [**IEEE TPAMI, 2025**] Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments [paper](https://ieeexplore.ieee.org/document/11106272/)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Information Fusion, 2024**] Embodied navigation with multi-modal information: A survey from tasks to methodology [paper](https://doi.org/10.1016/j.inffus.2024.102532)
* [**IEEE Transactions on Automation Science and Engineering, 2024**] A Survey of Object Goal Navigation [paper](https://ieeexplore.ieee.org/document/10475904/)
* [**ICRA, 2024**] Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-End Deep Reinforcement Learning [paper](https://ieeexplore.ieee.org/document/10611499/)
* [**ICRA, 2024**] UIVNav: Underwater Information-driven Vision-Based Navigation via Imitation Learning [paper](https://ieeexplore.ieee.org/document/10611203/)
* [**Information Fusion, 2024**] MACNS: A Generic Graph Neural Network Integrated Deep Reinforcement Learning Based Multi-Agent Collaborative Navigation System for Dynamic Trajectory Planning [paper](https://doi.org/10.1016/j.inffus.2024.102250)

#### 2.2.3 Embodied Question Answering (EQA)

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering [paper](https://arxiv.org/abs/2502.00342)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2024**] GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering [paper](https://arxiv.org/abs/2412.14480)
* [**CVPR, 2024**] OpenEQA: Embodied Question Answering in the Era of Foundation Models [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf)

### 2.3 Embodied Action

#### 2.3.1 Imitation Learning-Based Action

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE Transactions on Industrial Electronics, 2025**] Deep Multimodal Imitation Learning-Based Framework for Robot-Assisted Medical Examination [paper](https://shura.shu.ac.uk/36262/1/Si_2025_TIE_Deep_Multimodal_Imitation_Learning_based_Framework_for_Robot_assisted_Medical_Examination_final.pdf)
* [**ICRA, 2025**] EgoMimic: Scaling Imitation Learning via Egocentric Video [paper](https://ieeexplore.ieee.org/document/11127989/)
* [**IEEE Robotics and Automation Letters, 2025**] Stable-BC: Controlling Covariate Shift With Stable Behavior Cloning [paper](https://ieeexplore.ieee.org/document/10829660/)
* [**AAAI, 2025**] Inverse Reinforcement Learning by Estimating Expertise of Demonstrators [paper](https://ojs.aaai.org/index.php/AAAI/article/view/33705)
* [**ICLR, 2025**] Understanding Constraint Inference in Safety-Critical Inverse Reinforcement Learning [paper](https://openreview.net/forum?id=B2RXwASSpy)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE Transactions on Cybernetics, 2024**] A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges [paper](https://ieeexplore.ieee.org/document/10602544/)
* [**NeurIPS, 2024**] Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning [paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/da84e39ae51fd26bb5110d9659c06e13-Paper-Conference.pdf)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Foundations and Trends in Robotics, 2018**] An Algorithmic Perspective on Imitation Learning [paper](https://doi.org/10.1561/2300000053)

#### 2.3.2 Reinforcement Learning-Based Action

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**AAAI, 2025**] Deep reinforcement learning for robotics: A survey of real-world successes [paper](https://doi.org/10.1609/aaai.v39i27.35095)
* [**AAAI, 2025**] Autonomous option invention for continual hierarchical reinforcement learning and planning [paper](https://doi.org/10.1609/aaai.v39i18.34163)
* [**IEEE Transactions on Intelligent Transportation Systems, 2025**] Toward adaptive and coordinated transportation systems: A multi-personality multi-agent meta-reinforcement learning framework [paper](https://ieeexplore.ieee.org/document/10974402/)
* [**Nature Machine Intelligence, 2025**] Model-based reinforcement learning for ultrasound-driven autonomous microrobots [paper](https://doi.org/10.1038/s42256-025-01054-2)
* [**Artificial Intelligence for Engineers, 2025**] Value-based reinforcement learning [paper](https://doi.org/10.1007/978-3-031-75953-6_14)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**IEEE Transactions on Neural Networks and Learning Systems, 2022**] Deep reinforcement learning: A survey [paper](https://doi.org/10.1109/TNNLS.2022.3207346)
* [**ICML, 2018**] Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor [paper](https://proceedings.mlr.press/v80/haarnoja18b.html)
* [**arXiv, 2017**] Proximal policy optimization algorithms [paper](https://arxiv.org/abs/1707.06347)
* [**ICML, 2016**] Asynchronous methods for deep reinforcement learning [paper](https://proceedings.mlr.press/v48/mniha16.html)
* [**arXiv, 2015**] Continuous control with deep reinforcement learning [paper](https://arxiv.org/abs/1509.02971)
* [**Journal of Artificial Intelligence Research, 1996**] Reinforcement learning: A survey [paper](https://doi.org/10.1613/jair.301)
* [**Machine Learning, 1992**] Q-learning [paper](https://doi.org/10.1007/BF00992698)

#### 2.3.3 Large Model-Driven Action

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] Embodied AI Agents: Modeling the World [paper](https://arxiv.org/abs/2506.22355)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**arXiv, 2023**] GPT-4 Technical Report [paper](https://arxiv.org/abs/2303.08774)
* [**CoRL, 2023**] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [paper](https://proceedings.mlr.press/v229/zitkovich23a.html)
* [**arXiv, 2023**] PaLM-E: An Embodied Multimodal Language Model [paper](https://arxiv.org/abs/2303.03378)
* [**ICML, 2023**] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models [paper](https://proceedings.mlr.press/v202/li23q.html)
* [**NeurIPS, 2022**] Flamingo: a Visual Language Model for Few-Shot Learning [paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)
* [**arXiv, 2022**] Code as Policies: Language Model Programs for Embodied Control [paper](https://arxiv.org/abs/2209.07753)

<div align="center">
<a href="#top">‚¨Ü Back to top</a>
</div>

## üè• 3. Embodied AI in Medicine

<div align=center>
<img src="img\Fig2.png">
</div>

Embodied AI in medicine. Corresponding to the core components of embodied AI, med ical embodied AI encompasses medical embodied perception, medical embodied decision-making,  and medical embodied action. a, Medical embodied perception includes medical instrument and  organ recognition, perception and modeling of surgical and clinical environments, detection of medi cal operational behaviors, and understanding of affective and interactive cues. b, Medical embodied  decision-making encompasses medical workflow modeling and task planning, medical navigation sys tems, and clinical question-answering and decision-support mechanisms. c, Medical embodied action  consists of imitation-based medical actions, reinforcement-based medical actions, and large-model driven medical actions.

### 3.1 Medical Embodied Perception

#### 3.1.1 Medical Instrument and Organ Recognition

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] SurgVLM: A large vision-language model and systematic evaluation benchmark for surgical intelligence [paper](https://arxiv.org/abs/2506.02555)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Artificial Intelligence Review, 2024**] Deep learning for surgical instrument recognition and segmentation in robotic-assisted surgeries: a systematic review [paper](https://link.springer.com/article/10.1007/s10462-024-10979-w)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**IEEE Transactions on Neural Networks and Learning Systems, 2022**] SwinPA-Net: Swin transformer-based multiscale feature pyramid aggregation network for medical image segmentation [paper](https://ieeexplore.ieee.org/document/9895210)
* [**Medical Image Analysis, 2021**] ST-MTL: Spatio-temporal multitask learning model to predict scanpath while tracking instruments in robotic surgery [paper](https://doi.org/10.1016/j.media.2020.101837)
* [**arXiv, 2020**] Deep learning in multi-organ segmentation [paper](https://arxiv.org/abs/2001.10619)
* [**IEEE Robotics and Automation Letters, 2019**] Deep learning based robotic tool detection and articulation estimation with spatio-temporal layers [paper](https://ieeexplore.ieee.org/document/8715379)

#### 3.1.2 Surgical and Clinical Environment Perception and Modeling

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IJCARS, 2025**] NeRF-OR: neural radiance fields for operating room scene reconstruction from sparse-view RGB-D videos [paper](https://link.springer.com/article/10.1007/s11548-024-03261-5)
* [**arXiv, 2025**] Spatial-ORMLLM: Improve spatial relation understanding in the operating room with multimodal large language model [paper](https://arxiv.org/abs/2508.08199)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Neurosurgical Focus, 2024**] Creation of a microsurgical neuroanatomy laboratory and virtual operating room: a preliminary study [paper](https://thejns.org/focus/view/journals/neurosurg-focus/56/1/article-pE6.xml)
* [**MICCAI, 2024**] Deform3DGS: Flexible deformation for fast surgical scene reconstruction with Gaussian Splatting [paper](https://doi.org/10.1007/978-3-031-72089-5_13)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**MICCAI, 2023**] LABRAD-OR: Lightweight memory scene graphs for accurate bimodal reasoning in dynamic operating rooms [paper](https://link.springer.com/chapter/10.1007/978-3-031-43996-4_29)
* [**MICCAI, 2022**] 4D-OR: Semantic scene graphs for OR domain modeling [paper](https://link.springer.com/chapter/10.1007/978-3-031-16449-1_45)
* [**arXiv, 2020**] A robotic 3D perception system for operating room environment awareness [paper](https://arxiv.org/abs/2003.09487)

#### 3.1.3 Medical Operation Behavior Detection

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Annals of the New York Academy of Sciences, 2025**] STANet: A surgical gesture recognition method based on spatiotemporal fusion [paper](https://doi.org/10.1111/nyas.70053)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE FG, 2024**] MGRFormer: A multimodal transformer approach for surgical gesture recognition [paper](https://hal.science/hal-04603132)
* [**ITNEC, 2024**] Surgical gesture recognition in open surgery based on 3DCNN and SlowFast [paper](https://ieeexplore.ieee.org/document/10733142/)
* [**CGI, 2024**] TransSG: A spatial-temporal transformer for surgical gesture recognition [paper](https://doi.org/10.1007/978-3-031-82024-3_12)
* [**IEEE Access, 2024**] Audio-and video-based human activity recognition systems in healthcare [paper](https://doi.org/10.1109/ACCESS.2024.3353138)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**IEEE JBHI, 2023**] Deep learning in surgical workflow analysis: a review of phase and step recognition [paper](https://doi.org/10.1109/JBHI.2023.3311628)
* [**IEEE Transactions on Medical Imaging, 2022**] Gesture recognition in robotic surgery with multimodal attention [paper](https://doi.org/10.1109/TMI.2022.3147640)

#### 3.1.4 Emotional Interaction Understanding

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Pattern Recognition, 2025**] Multimodal latent emotion recognition from micro-expression and physiological signal [paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320325006235)
* [**Information Fusion, 2025**] Towards facial micro-expression detection and classification using modified multimodal ensemble learning approach [paper](https://www.sciencedirect.com/science/article/abs/pii/S156625352400513X)
* [**IEEE JBHI, 2025**] Multimodal fusion of behavioral and physiological signals for enhanced emotion recognition via feature decoupling and knowledge transfer [paper](https://ieeexplore.ieee.org/abstract/document/11122276)
* [**MICCAI, 2025**] MedVLM-R1: Incentivizing medical reasoning capability of vision-language models (VLMs) via reinforcement learning [paper](https://doi.org/10.1007/978-3-032-04981-0_32)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE Transactions on Circuits and Systems for Video Technology, 2024**] Dep-former: Multimodal depression recognition based on facial expressions and audio features via emotional changes [paper](https://ieeexplore.ieee.org/document/10742391/)
* [**Expert Systems with Applications, 2024**] MSER: Multimodal speech emotion recognition using cross-attention with deep fusion [paper](https://www.sciencedirect.com/science/article/abs/pii/S0957417423034486)
* [**IEEE Transactions on Instrumentation and Measurement, 2024**] Deep learning-based automated emotion recognition using multimodal physiological signals and time-frequency methods [paper](https://ieeexplore.ieee.org/document/10579770/)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**arXiv, 2023**] DialogueLLM: Context and emotion knowledge-tuned large language models for emotion recognition in conversations [paper](https://arxiv.org/abs/2310.11374)
* [**IEEE Transactions on Affective Computing, 2022**] Deep learning for micro-expression recognition: A survey [paper](https://doi.org/10.1109/TAFFC.2022.3205170)
* [**IEEE Transactions on Affective Computing, 2020**] Deep facial expression recognition: A survey [paper](https://doi.org/10.1109/TAFFC.2020.2981446)

### 3.2 Medical Embodied Decision-Making

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**JAMA Surgery, 2020**] Artificial intelligence and surgical decision-making [paper](https://doi.org/10.1001/jamasurg.2019.4917)
* [**EBioMedicine, 2019**] Artificial intelligence to support clinical decision-making processes [paper](https://doi.org/10.1016/j.ebiom.2019.07.019)

#### 3.2.1 Medical Workflow Modeling and Task Planning

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**NeurIPS, 2023**] LLaVA-Med: Training a large language-and-vision assistant for biomedicine in one day [paper](https://doi.org/10.48550/arXiv.2306.00890)
* [**Journal of Dentistry, 2023**] Artificial intelligence and augmented reality for guided implant surgery planning: a proof of concept [paper](https://doi.org/10.1016/j.jdent.2023.104485)
* [**IJCARS, 2022**] PATG: position-aware temporal graph networks for surgical phase recognition on laparoscopic videos [paper](https://doi.org/10.1007/s11548-022-02600-8)
* [**IEEE-EMBS BHI, 2022**] Towards graph representation learning based surgical workflow anticipation [paper](https://doi.org/10.1109/BHI56158.2022.9926801)
* [**MICCAI, 2021**] Trans-SVNet: Accurate phase recognition from surgical videos via hybrid embedding aggregation transformer [paper](https://doi.org/10.1007/978-3-030-87202-1_57)
* [**Health Systems, 2021**] Clinical pathway modelling: a literature review [paper](https://doi.org/10.1080/20476965.2019.1652547)
* [**MICCAI, 2020**] TeCNO: Surgical phase recognition with multi-stage temporal convolutional networks [paper](https://doi.org/10.1007/978-3-030-59716-0_33)

#### 3.2.2 Medical Navigation Systems

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**International Journal of Nanomedicine, 2025**] Localized drug delivery in different gastrointestinal cancers: navigating challenges and advancing nanotechnological solutions [paper](https://doi.org/10.2147/IJN.S502833)
* [**IEEE Transactions on Automation Science and Engineering, 2025**] RL-USRegi: Autonomous ultrasound registration for radiation-free spinal surgical navigation using reinforcement learning [paper](https://ieeexplore.ieee.org/document/10897959/)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**ICSR, 2024**] Utilizing a social robot as a greeter at a children's hospital [paper](https://doi.org/10.1007/978-981-96-3525-2_11)
* [**IJCARS, 2024**] Autonomous navigation of catheters and guidewires in mechanical thrombectomy using inverse reinforcement learning [paper](https://doi.org/10.1007/s11548-024-03208-w)
* [**AAAI, 2024**] NavGPT: Explicit reasoning in vision-and-language navigation with large language models [paper](https://doi.org/10.1609/aaai.v38i7.28597)
* [**ECCV, 2024**] NavGPT-2: Unleashing navigational reasoning capability for large vision-language models [paper](https://doi.org/10.1007/978-3-031-72667-5_15)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Annals of Biomedical Engineering, 2021**] A wearable augmented reality navigation system for surgical telementoring based on Microsoft HoloLens [paper](https://doi.org/10.1007/s10439-020-02538-5)
* [**IPIN, 2015**] Navigating in large hospitals [paper](https://doi.org/10.1109/IPIN.2015.7346758)
* [**Langenbeck's Archives of Surgery, 2013**] Navigation in surgery [paper](https://doi.org/10.1007/s00423-013-1059-4)
* [**Neurosurgery, 1999**] BrainLab VectorVision neuronavigation system: technology and clinical experiences in 131 cases [paper](https://doi.org/10.1097/00006123-199901000-00056)

#### 3.2.3 Clinical Question-answering and Decision Support

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Nature Medicine, 2025**] Toward expert-level medical question answering with large language models [paper](https://doi.org/10.1038/s41591-024-03423-7)
* [**Nature Medicine, 2025**] Clinical implementation of an AI-based prediction model for decision support for patients undergoing colorectal cancer surgery [paper](https://doi.org/10.1038/s41591-025-03942-x)
* [**BMC Oral Health, 2025**] Artificial intelligence-based chatbot assistance in clinical decision-making for medically complex patients in oral surgery: a comparative study [paper](https://doi.org/10.1186/s12903-025-05732-w)
* [**Research Square, 2025**] Multimodal decision support system for improved diagnosis and healthcare decision making [paper](https://www.researchsquare.com/article/rs-6430452/v1)
* [**Computerized Medical Imaging and Graphics, 2025**] MedBLIP: A multimodal method of medical question-answering based on fine-tuning large language model [paper](https://doi.org/10.1016/j.compmedimag.2025.102581)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Artificial Intelligence in Medicine, 2023**] Medical visual question answering: A survey [paper](https://doi.org/10.1016/j.artmed.2023.102611)
* [**ACM Computing Surveys, 2022**] Biomedical question answering: a survey of approaches and challenges [paper](https://doi.org/10.1145/3490238)

### 3.3 Medical Embodied Action

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Science Robotics, 2025**] Will your next surgeon be a robot? Autonomy and AI in robotic surgery [paper](https://doi.org/10.1126/scirobotics.adt0187)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Annual Review of Control, Robotics, and Autonomous Systems, 2021**] Autonomy in surgical robotics [paper](https://doi.org/10.1146/annurev-control-062420-090543)

#### 3.3.1 Medical Imitation-based Action

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE Transactions on Biomedical Engineering, 2025**] Imitation learning for path planning in cardiac percutaneous interventions [paper](https://doi.org/10.1109/TBME.2025.3542224)
* [**arXiv, 2025**] SuFIA-BC: Generating High Quality Demonstration Data for Visuomotor Policy Learning in Surgical Subtasks [paper](https://arxiv.org/abs/2504.14857)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2024**] Surgical Robot Transformer (SRT): Imitation Learning for Surgical Tasks [paper](https://arxiv.org/abs/2407.12998)
* [**IROS, 2024**] Toward a surgeon-in-the-loop ophthalmic robotic apprentice using reinforcement and imitation learning [paper](https://arxiv.org/abs/2311.17693)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**ICRA, 2022**] 3D perception based imitation learning under limited demonstration for laparoscope control in robotic surgery [paper](https://doi.org/10.1109/ICRA46639.2022.9812010)
* [**ICRA, 2021**] Intermittent visual servoing: Efficiently learning policies robust to instrument changes for high-precision surgical manipulation [paper](https://doi.org/10.1109/ICRA48506.2021.9561070)
* [**IEEE Transactions on Biomedical Engineering, 2021**] Inverse reinforcement learning intra-operative path planning for steerable needle [paper](https://doi.org/10.1109/TBME.2021.3133075)
* [**ICRA, 2020**] Collaborative robot-assisted endovascular catheterization with generative adversarial imitation learning [paper](https://doi.org/10.1109/ICRA40945.2020.9196912)

#### 3.3.2 Medical Reinforcement-based Action

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2024**] Surgical Task Automation Using Actor-Critic Frameworks and Self-Supervised Imitation Learning [paper](https://arxiv.org/abs/2409.02724)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Journal of Machine Learning Research, 2023**] LapGym - An Open Source Framework for Reinforcement Learning in Robot-Assisted Laparoscopic Surgery [paper](https://jmlr.org/papers/v24/23-0207.html)
* [**IEEE Transactions on Industrial Electronics, 2023**] CASOG: Conservative actor‚Äìcritic with smooth gradient for skill learning in robot-assisted intervention [paper](https://ieeexplore.ieee.org/document/10254299)
* [**ICMA, 2022**] Evaluation of an autonomous navigation method for vascular interventional surgery in a virtual environment [paper](https://doi.org/10.1109/ICMA54519.2022.9856107)
* [**ACM Computing Surveys, 2021**] Reinforcement learning in healthcare: A survey [paper](https://doi.org/10.1145/3477600)
* [**RO-MAN, 2020**] Collaborative suturing: A reinforcement learning approach to automate hand-off task in suturing for surgical robots [paper](https://doi.org/10.1109/RO-MAN47096.2020.9223543)

#### 3.3.3 Medical Large Model-Driven Action

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Nature Reviews Electrical Engineering, 2025**] Innovating robot-assisted surgery through large vision models [paper](https://doi.org/10.1038/s44287-025-00166-6)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2024**] GP-VLS: A general-purpose vision language model for surgery [paper](https://arxiv.org/abs/2407.19305)
* [**Artificial Intelligence Review, 2024**] Large language models in healthcare: from a systematic review on medical examinations to a comparative analysis on fundamentals of robotic surgery online test [paper](https://doi.org/10.1007/s10462-024-10849-5)
* [**arXiv, 2024**] RoboNurse-VLA: Robotic scrub nurse system based on vision-language-action model [paper](https://arxiv.org/abs/2409.19590)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**MICCAI, 2023**] SurgicalGPT: end-to-end language-vision GPT for visual question answering in surgery [paper](https://doi.org/10.1007/978-3-031-43996-4_27)
* [**ML4H, 2023**] Med-Flamingo: a multimodal medical few-shot learner [paper](https://proceedings.mlr.press/v225/moor23a.html)

### 3.4 Integrated Application Scenarios in Healthcare

#### 3.4.1 Surgical Robot

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Surgical Innovation, 2024**] The use of the Symani Surgical System¬Æ in emergency hand trauma care [paper](https://doi.org/10.1177/15533506241262568)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**IEEE Robotics & Automation Magazine, 2021**] Accelerating surgical robotics research: A review of 10 years with the da Vinci research kit [paper](https://doi.org/10.1109/MRA.2021.3101646)
* [**ICRA, 2019**] Robotic bronchoscopy drive mode of the Auris Monarch platform [paper](https://doi.org/10.1109/ICRA.2019.8793704)
* [**Journal of Neurosurgery, 2019**] Neuroendovascular-specific engineering modifications to the CorPath GRX robotic system [paper](https://doi.org/10.3171/2019.9.JNS192113)
* [**Nature Biomedical Engineering, 2018**] First-in-human study of the safety and viability of intraocular robotic surgery [paper](https://doi.org/10.1038/s41551-018-0248-4)
* [**European Archives of Oto-Rhino-Laryngology, 2015**] Transoral robotic surgery (TORS) with the Medrobotics Flex‚Ñ¢ system: first surgical application on humans [paper](https://doi.org/10.1007/s00405-015-3532-x)
* [**IJMRCAS, 2015**] A pneumatic laparoscope holder controlled by head movement [paper](https://doi.org/10.1002/rcs.1606)
* [**Technology in Cancer Research & Treatment, 2010**] The CyberKnife¬Æ robotic radiosurgery system in 2010 [paper](https://doi.org/10.1177/153303461000900502)
* [**Intuitive Surgical, 2013**] da Vinci surgical system [paper](https://www.intuitive.com/en-us/products-and-services/da-vinci)
* [**IJMRCAS, 2013**] Technical review of the da Vinci surgical telemanipulator [paper](https://doi.org/10.1002/rcs.1468)
* [**Clinical Orthopaedics and Related Research, 1998**] Primary and revision total hip replacement using the ROBODOC (R) system [paper](https://doi.org/10.1097/00003086-199809000-00011)

#### 3.4.2 Intelligent Caregiving and Companion Robot

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Envisioning the Future of Health Informatics and Digital Health, 2025**] The Pepper robot in healthcare: A scoping review [paper](https://ebooks.iospress.nl/DOI/10.3233/SHTI250057)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Journal of Aging Research & Lifestyle, 2024**] ElliQ, an AI-driven social robot to alleviate loneliness: progress and lessons learned [paper](https://doi.org/10.14283/jarlife.2024.2)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**European Journal of Pediatrics, 2022**] The pilot study of group robot intervention on pediatric inpatients and their caregivers, using 'new aibo' [paper](https://doi.org/10.1007/s00431-021-04285-8)
* [**Journal of Engineering in Medicine, 2018**] Arash: A social robot buddy to support children with cancer in a hospital environment [paper](https://doi.org/10.1177/0954411918777520)
* [**IJERPH, 2020**] Robotics utilization for healthcare digitization in global COVID-19 management [paper](https://doi.org/10.3390/ijerph17113819)
* [**BMC Geriatrics, 2019**] The benefits of and barriers to using a social robot PARO in care settings: a scoping review [paper](https://doi.org/10.1186/s12877-019-1244-6)
* [**RO-MAN, 2012**] Technical improvements of the Giraff telepresence robot based on users' evaluation [paper](https://doi.org/10.1109/ROMAN.2012.6343854)
* [**SIGGRAPH Emerging Technologies, 2011**] Telenoid: Tele-presence android for communication [paper](https://doi.org/10.1145/2048259.2048274)

#### 3.4.3 Immersive Medical Education Platform

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Digital Diagnostics, 2024**] Possibilities for using the Vimedix 3.2 virtual simulator to train ultrasound specialists [paper](https://jdigitaldiagnostics.com/DD/article/view/586551)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**CVPR, 2022**] OSSO: Obtaining skeletal shape from outside [paper](https://openaccess.thecvf.com/content/CVPR2022/html/Keller_OSSO_Obtaining_Skeletal_Shape_From_Outside_CVPR_2022_paper.html)
* [**JMLA, 2022**] 3D Organon VR Anatomy [paper](https://doi.org/10.5195/jmla.2022.1341)
* [**Surgical Technology International, 2019**] Validation of the hip arthroscopy module of the VirtaMed virtual reality arthroscopy trainer [paper](https://pubmed.ncbi.nlm.nih.gov/30753742/)
* [**PhD Thesis/Report, 2020**] The first experience of using the Body Interact simulation interactive training platform as a part of interns' attestation [paper](https://doi.org/10.11603/me.2414-5998.2020.2.11150)
* [**Handbook of Research on Educational Communications and Technology, 2008**] Cognitive task analysis [paper](https://doi.org/10.4324/9780203880869-48)

#### 3.4.4 Telecollaborative Diagnostic and Treatment System

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Web, 2025**] Teladoc Health [paper](https://www.teladochealth.com/)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Frontiers in Human Neuroscience, 2024**] A retrospective, observational study of real-world clinical data from the cognitive function development therapy program [paper](https://doi.org/10.3389/fnhum.2024.1508815)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Nurse Leader, 2016**] Mercy virtual nursing: An innovative care delivery model [paper](https://doi.org/10.1016/j.mnl.2016.05.011)
* [**Molecular Nutrition & Food Research, 2019**] Duck egg white‚Äìderived peptide VSEE (Val-Ser-Glu-Glu) regulates bone and lipid metabolisms by Wnt/Œ≤-catenin signaling pathway and intestinal microbiota [paper](https://doi.org/10.1002/mnfr.201900525)

<div align="center">
<a href="#top">‚¨Ü Back to top</a>
</div>

## üìä 4. Datasets and benchmark

### 4.1 Perception Datasets

#### 4.1.1 Organ‚ÄìInstrument Recognition Datasets

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] Tumor detection, segmentation and classification challenge on automated 3D breast ultrasound: The TDSC-ABUS challenge [paper](https://arxiv.org/abs/2501.15588)
* [**Medical Image Analysis, 2025**] The ULS23 challenge: A baseline model and benchmark dataset for 3D universal lesion segmentation in computed tomography [paper](https://doi.org/10.1016/j.media.2025.103525)
* [**CVPR, 2025**] CholecTrack20: A multi-perspective tracking dataset for surgical tools [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Nwoye_CholecTrack20_A_Multi-Perspective_Tracking_Dataset_for_Surgical_Tools_CVPR_2025_paper.html)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Scientific Data, 2024**] Lumbar spine segmentation in MR images: a dataset and a public benchmark [paper](https://doi.org/10.1038/s41597-024-03090-w)
* [**arXiv, 2024**] SegSTRONG-C: Segmenting surgical tools robustly on non-adversarial generated corruptions ‚Äì an EndoVis'24 challenge [paper](https://arxiv.org/abs/2407.11906)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Radiology: Artificial Intelligence, 2023**] TotalSegmentator: robust segmentation of 104 anatomic structures in CT images [paper](https://doi.org/10.1148/ryai.230024)
* [**arXiv, 2023**] The state-of-the-art 3D anisotropic intracranial hemorrhage segmentation on non-contrast head CT: The INSTANCE challenge [paper](https://arxiv.org/abs/2301.03281)
* [**arXiv, 2023**] FairSeg: A large-scale medical image segmentation dataset for fairness learning using segment anything model with fair error-bound scaling [paper](https://arxiv.org/abs/2311.02189)
* [**Scientific Data, 2022**] A whole-body FDG-PET/CT dataset with manually annotated tumor lesions [paper](https://doi.org/10.1038/s41597-022-01718-3)
* [**Medical Image Analysis, 2022**] AtrialJSQnet: a new framework for joint segmentation and quantification of left atrium and scars incorporating spatial and shape information [paper](https://doi.org/10.1016/j.media.2021.102303)
* [**arXiv, 2021**] CTSpine1K: A large-scale dataset for spinal vertebrae segmentation in computed tomography [paper](https://arxiv.org/abs/2105.14711)
* [**IJCARS, 2021**] Deep learning to segment pelvic bones: large-scale CT datasets and baseline models [paper](https://doi.org/10.1007/s11548-021-02363-8)
* [**Medical Image Analysis, 2021**] VerSe: a vertebrae labelling and segmentation benchmark for multi-detector CT images [paper](https://doi.org/10.1016/j.media.2021.102166)
* [**arXiv, 2020**] m2caiSeg: Semantic segmentation of laparoscopic images using convolutional neural networks [paper](https://arxiv.org/abs/2008.10134)
* [**IROS, 2020**] LC-GAN: Image-to-image translation based on generative adversarial network for endoscopic images [paper](https://doi.org/10.1109/IROS45743.2020.9341556)
* [**IPTA, 2020**] SegTHOR: Segmentation of thoracic organs at risk in CT images [paper](https://arxiv.org/abs/1912.05950)
* [**arXiv, 2019**] 2017 robotic instrument segmentation challenge [paper](https://arxiv.org/abs/1902.06426)
* [**IEEE Transactions on Medical Imaging, 2016**] EndoNet: a deep architecture for recognition tasks on laparoscopic videos [paper](https://doi.org/10.1109/TMI.2016.2593957)
* [**arXiv, 2016**] Skin lesion analysis toward melanoma detection: A challenge at the international symposium on biomedical imaging (ISBI) 2016 [paper](https://arxiv.org/abs/1605.01397)
* [**Expert Systems with Applications, 2015**] MED-NODE: A computer-assisted melanoma diagnosis system using non-dermoscopic images [paper](https://doi.org/10.1016/j.eswa.2015.04.034)
* [**Dermoscopy Image Analysis, 2015**] PH2: A public database for the analysis of dermoscopic images [paper](https://www.taylorfrancis.com/chapters/mono/10.1201/b19107-17/ph2-public-database-analysis-dermoscopic-images)
* [**Web, 2010**] 3D image reconstruction for comparison of algorithm database (3D-IRCADb-01) [paper](https://www.ircad.fr/research/data-sets/liver-segmentation-3d-ircadb-01/)

#### 4.1.2 Medical Scene Modeling Datasets

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**CVPR, 2025**] MM-OR: A large multimodal operating room dataset for semantic understanding of high-intensity surgical environments [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Ozsoy_MM-OR_A_Large_Multimodal_Operating_Room_Dataset_for_Semantic_Understanding_CVPR_2025_paper.html)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Engineering Applications of Artificial Intelligence, 2023**] Object detection in hospital facilities: A comprehensive dataset and performance evaluation [paper](https://doi.org/10.1016/j.engappai.2023.106223)
* [**Data in Brief, 2020**] MyNursingHome: A fully-labelled image dataset for indoor object classification [paper](https://doi.org/10.1016/j.dib.2020.106268)
* [**IEEE Transactions on Biomedical Engineering, 2016**] See it with your own eyes: Markerless mobile augmented reality for radiation awareness in the hybrid room [paper](https://doi.org/10.1109/TBME.2016.2560761)
* [**Data in Brief, 2018**] MCIndoor20000: A fully-labeled image dataset to advance indoor objects detection [paper](https://doi.org/10.1016/j.dib.2017.12.047)

#### 4.1.3 Clinical Action and Pose Estimation Datasets

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**MICCAI Workshops, 2025**] SurgTrack: CAD-free 3D tracking of real-world surgical instruments [paper](https://doi.org/10.1007/978-3-031-77610-6_16)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Algorithms, 2024**] MMD-MSD: A multimodal multisensory dataset in support of research and technology development for musculoskeletal disorders [paper](https://doi.org/10.3390/a17050187)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**arXiv, 2018**] MVOR: A multi-view RGB-D operating room dataset for 2D and 3D human pose estimation [paper](https://arxiv.org/abs/1808.08180)
* [**IEEE Journal of Translational Engineering in Health and Medicine, 2018**] Patient-specific pose estimation in clinical environments [paper](https://doi.org/10.1109/JTEHM.2018.2875464)

#### 4.1.4 Multimodal Affective Perception Datasets

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] Advancing face-to-face emotion communication: A multimodal dataset (AFFEC) [paper](https://arxiv.org/abs/2504.18969)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**NeurIPS, 2024**] Emotion-LLaMA: Multimodal emotion recognition and reasoning with instruction tuning [paper](https://arxiv.org/abs/2406.11161)
* [**Scientific Data, 2024**] A multimodal dataset for mixed emotion recognition [paper](https://doi.org/10.1038/s41597-024-03676-4)
* [**IEEE Transactions on Affective Computing, 2024**] SEED-VII: A multimodal dataset of six basic emotions with continuous labels for emotion recognition [paper](https://doi.org/10.1109/TAFFC.2024.3485057)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**IEEE JBHI, 2017**] DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices [paper](https://doi.org/10.1109/JBHI.2017.2688239)
* [**IEEE Transactions on Affective Computing, 2016**] ASCERTAIN: Emotion and personality recognition using commercial sensors [paper](https://doi.org/10.1109/TAFFC.2016.2625250)

### 4.2 Decision-Making Datasets

#### 4.2.1 Surgical Workflow Annotation Datasets

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Scientific Data, 2025**] LapEx: A new multimodal dataset for context recognition and practice assessment in laparoscopic surgery [paper](https://doi.org/10.1038/s41597-025-04588-7)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**ECCV, 2024**] OphNet: A large-scale video benchmark for ophthalmic surgical workflow understanding [paper](https://doi.org/10.1007/978-3-031-73235-5_27)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**MICCAI, 2022**] AutoLaparo: A new dataset of integrated multi-tasks for image-guided surgical automation in laparoscopic hysterectomy [paper](https://doi.org/10.1007/978-3-031-16449-1_46)
* [**Computer Methods and Programs in Biomedicine, 2021**] Micro-surgical anastomose workflow recognition challenge report [paper](https://doi.org/10.1016/j.cmpb.2021.106452)

#### 4.2.2 Medical Navigation Datasets

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IEEE Sensors Journal, 2025**] A portable 6D surgical instrument magnetic localization system with dynamic error correction [paper](https://ieeexplore.ieee.org/abstract/document/11040120)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**Scientific Data, 2024**] Head model dataset for mixed reality navigation in neurosurgical interventions for intracranial lesions [paper](https://doi.org/10.1038/s41597-024-03385-y)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**arXiv, 2021**] Habitat-Matterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI [paper](https://arxiv.org/abs/2109.08238)
* [**CVPR, 2018**] Gibson Env: Real-World Perception for Embodied Agents [paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Xia_Gibson_Env_Real-World_CVPR_2018_paper.html)

#### 4.2.3 Medical Question Answering Datasets

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] MedReason: Eliciting factual medical reasoning steps in LLMs via knowledge graphs [paper](https://arxiv.org/abs/2504.00993)
* [**EMNLP, 2025**] ReasonMed: A 370k multi-agent generated dataset for advancing medical reasoning [paper](https://aclanthology.org/2025.emnlp-main.1344/)
* [**arXiv, 2025**] ORQA: A benchmark and foundation model for holistic operating room modeling [paper](https://arxiv.org/abs/2505.12890)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IJCARS, 2024**] Advancing surgical VQA with scene graph knowledge [paper](https://doi.org/10.1007/s11548-024-03141-y)
* [**arXiv, 2024**] ERVQA: A dataset to benchmark the readiness of large vision language models in hospital environments [paper](https://arxiv.org/abs/2410.06420)
* [**arXiv, 2024**] LLaVA-Surg: towards multimodal surgical assistant via structured surgical video learning [paper](https://arxiv.org/abs/2408.07981)

### 4.3 Action Datasets

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] Surgical visual understanding (SurgVU) dataset [paper](https://arxiv.org/abs/2501.09209)
* [**ACM Multimedia, 2025**] CoPESD: A multi-level surgical motion dataset for training large vision-language models to co-pilot endoscopic submucosal dissection [paper](https://dl.acm.org/doi/10.1145/3746027.3758200)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**IJCARS, 2024**] Challenges in multi-centric generalization: phase and step recognition in Roux-en-Y gastric bypass surgery [paper](https://doi.org/10.1007/s11548-024-03166-3)
* [**arXiv, 2024**] General surgery vision transformer: A video pre-trained foundation model for general surgery [paper](https://arxiv.org/abs/2403.05949)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**arXiv, 2023**] The EndoScapes dataset for surgical scene segmentation, object detection, and critical view of safety assessment [paper](https://arxiv.org/abs/2312.12429)
* [**arXiv, 2022**] MITI: SLAM benchmark for laparoscopic surgery [paper](https://arxiv.org/abs/2202.11496)
* [**arXiv, 2016**] The TUM LapChole dataset for the M2CAI 2016 workflow challenge [paper](https://arxiv.org/abs/1610.09278)
* [**MICCAI Workshop, 2014**] JHU-ISI gesture and skill assessment working set (JIGSAWS): A surgical activity dataset for human motion modeling [paper](https://cirl.lcsr.jhu.edu/wp-content/uploads/2015/11/JIGSAWS.pdf)

### 4.4 Simulation Platforms and Synthetic Datasets

#### 4.4.1 Surgical Simulation Platforms

<span style="color: red"><strong>2025</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**arXiv, 2025**] SonoGym: High performance simulation for challenging surgical tasks with robotic ultrasound [paper](https://arxiv.org/abs/2507.01152)

<span style="color: red"><strong>2024</strong></span> <span style="margin-left:6px">üìÖ</span>

* [**ICRA, 2024**] Orbit-Surgical: An open-simulation framework for learning surgical augmented dexterity [paper](https://arxiv.org/abs/2404.16027)
* [**ICRA, 2024**] Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots [paper](https://arxiv.org/abs/2310.04676)

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Journal of Machine Learning Research, 2023**] LapGym - an open source framework for reinforcement learning in robot-assisted laparoscopic surgery [paper](https://jmlr.org/papers/v24/23-0207.html)
* [**IROS, 2021**] SurRoL: An open-source reinforcement learning centered and dVRK compatible platform for surgical robot learning [paper](https://arxiv.org/abs/2108.13035)

#### 4.4.2 Synthetic Datasets

<span style="color: red"><strong>archive</strong></span> <span style="margin-left:6px">üóÇÔ∏è</span>

* [**Scientific Data, 2023**] A large-scale synthetic pathological dataset for deep learning-enabled segmentation of breast cancer [paper](https://doi.org/10.1038/s41597-023-02125-y)
* [**arXiv, 2023**] SynFundus-1M: a high-quality million-scale synthetic fundus images dataset with fifteen types of annotation [paper](https://arxiv.org/abs/2312.00377)

* [**Electronics, 2022**] The ‚Äúcoherent data set‚Äù: combining patient data and imaging in a comprehensive, synthetic health record [paper](https://doi.org/10.3390/electronics11081199)
* [**Intelligence-Based Medicine, 2020**] Synthea‚Ñ¢ novel coronavirus (COVID-19) model and synthetic data set [paper](https://doi.org/10.1016/j.ibmed.2020.100007)

---

<div align="center">
<a href="#top">‚¨Ü Back to top</a>
</div>

